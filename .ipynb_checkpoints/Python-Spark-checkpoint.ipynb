{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "- Install GOW\n",
    "    - [https://github.com/bmatzelle/gow/releases](https://github.com/bmatzelle/gow/releases)\n",
    "- Install Spark\n",
    "    - [https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz)\n",
    "    - create a directory to hold the spark installation, and move the downloaded .tgz file there\n",
    "    - navigate to that directory in the command prompt\n",
    "    - upzip using `gzip -d filename.tgz`\n",
    "    - untar using `tar xvf filename.tar`\n",
    "    - navigate to the new spark folder in the cmd prompt\n",
    "    - cd into bin and add winutils using command below\n",
    "    - `curl -k -L -o winutils.exe https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin/winutils.exe?raw=true`\n",
    "    - ensure Java 8 is installed to path [https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3](https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3)\n",
    "    - in command prompt, set spark variables\n",
    "    - `setx SPARK_HOME C:\\Opt\\spark\\spark-3.0.0-preview2-bin-hadoop3.2`\n",
    "    - `setx HADOOP_HOME C:\\Opt\\spark\\spark-3.0.0-preview2-bin-hadoop3.2`\n",
    "    - `setx PYSPARK_DRIVER_PYTHON iptyhon` try jupyter here if issues\n",
    "    - `setx PYSPARK_DRIVER_PYTHON_OPTS notebook`\n",
    "    - add `C:\\Opt\\spark\\spark-3.0.0-preview2-bin-hadoop3.2\\bin\\` to the PATH variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Framework\n",
    "- Platform for cluster computing\n",
    "- Data is spread over multiple nodes in each cluster\n",
    "    - each node only handles a small amount of data, increasing speed\n",
    "    - results in parallel computing, as computations are carried out simultaneously\n",
    "- Considerations\n",
    "    - is my data too big for one machine?\n",
    "    - can my computations be easily parallelized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a Cluster\n",
    "- The master computer is connected to all other nodes\n",
    "    - this manages the data and splits up the calculations\n",
    "- The other computers in the cluster are workers\n",
    "    - workers get data and calculations to run from the master\n",
    "    - they return the results to the master\n",
    "- Create an instance of the `SparkContext` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Data Structures\n",
    "- The Resilient Distributed Dataset (RDD) is spark's core data structure\n",
    "    - this lets spark split the data across multiple nodes in a cluster\n",
    "    - RDD's are hard to work with directly\n",
    "- Spark Dataframe abstraction\n",
    "    - built on top of RDD\n",
    "    - easier to use than direct RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Spark Dataframes\n",
    "- Create a `SparkSession` from the `SparkContext`\n",
    "    - `SparkContext` is the connection to the cluster\n",
    "    - `SparkSession` is the interface to that connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark Session\n",
    "- Creating multiple sessions and contexts can cause issues\n",
    "    - the `SparkSession.builder.getOrCreate()` method will return an existing session if there is one, and create one if there isn't one yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
