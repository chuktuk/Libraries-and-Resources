{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics\n",
    "- Suite of algorithms and techniques that learn from data, find patterns, and make predictions\n",
    "- Useful with multivariate systems that are too complicated to use classical statistics\n",
    "- Two main types\n",
    "    - Supervised Learning\n",
    "        - learning from labeled examples (categories)\n",
    "        - kNN, SVM, Decision Trees, Random Forests, Bagging, Boosting, etc.\n",
    "        - make predictions for new data points\n",
    "    - Unsupervised Learning\n",
    "        - learning from unlabeled data to create categories/clusters\n",
    "        - PCA, MDS, Clustering\n",
    "        - find patterns in data\n",
    "        - for data with no categories (maybe don't know what they are), don't have training data, can't label because it's so big, don't know what's in it, or need to identify patterns/structure\n",
    "- Learn by example\n",
    "    - training data set\n",
    "- Dividing the data\n",
    "    - Production Models\n",
    "        - 60% train, 20% validation, 20% test\n",
    "    - Other methods\n",
    "        - **30%** test data (though I've seen examples with more like 15% test data, depending on size of the dataset)\n",
    "        - divide training data into equal sized folds for cross validation and hyperparameter estimation\n",
    "        - use the test data once at the very end to evaluate the model performance\n",
    "- Loss function\n",
    "    - this is the 'error function' that is trying to be minimized by the model\n",
    "    - in linear regression, this may be a least squares function (OLS) (sum of squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression vs. Classification\n",
    "- Both supervised\n",
    "    - regression -> target predicted variable is continuous\n",
    "    - classification -> target predicted variable is categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "- Selection Bias\n",
    "    - where did the data come from, and what are you missing\n",
    "    - your sample vs. the population\n",
    "- Publication Bias\n",
    "    - only positive/significant results published\n",
    "    - highly influence by $\\alpha < 0.05$ threshold\n",
    "- Non-Response Bias\n",
    "    - people did/didn't respond are different\n",
    "- Length Bias\n",
    "    - especially important with time based measurements\n",
    "    - bias towards individuals (data points) that remain in a specific state longer\n",
    "        - you miss the two day sample and collect the 20 yr sample, this happens repeatedly\n",
    "- Calculation\n",
    "    - difference between your estimation and the \"truth\"\n",
    "        - almost impossible to find the \"truth\", so really can't calculate bias accurately\n",
    "- MSE (meas squared error)\n",
    "    - $MSE = variance + bias^2$\n",
    "    - decreasing bias often means taking more samples or increasing model complexity, which means increasing variance\n",
    "    - it's a tradeoff, and decreasing bias may increase variance enough to make the MSE very large\n",
    "    - want to minimize MSE, rather than bias\n",
    "- Fisher Weighting\n",
    "    - way to take multiple independent unbiased estimators and combine into one number\n",
    "    - take a weighted average\n",
    "        - individual weights add up to one\n",
    "        - individual weights inversely proportional to the variance\n",
    "- Nate Silver Weighting\n",
    "    - take each variable into account based on particular circumstances\n",
    "        - past reliability, known biases, etc.\n",
    "- Bonferroni Correction\n",
    "    - divide $\\alpha$/number of tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression toward the mean (RTTM)\n",
    "    - predicting things like son's heights based on father's heights\n",
    "        - the son won't be exactly the same height as the father, and will likely be between the father's height and the mean\n",
    "        - combination of luck/skill (luck = chance, skill = effect)\n",
    "        - example praising good performance leads to worse performance and punishing bad performance leads to better performance:\n",
    "            - RTTM would predict bad to get better and good to get worse in many cases just by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS (Ordinary Least Squares)\n",
    "- defines a linear model (the model is not OLS, that is the procedure to generate the linear model)\n",
    "- **always plot the residuals!**\n",
    "    - no pattern is a good thing\n",
    "    - heteroscedasticity: bigger spread on one side\n",
    "    - nonlinear: distinct pattern to the residuals (U-shape or similar)\n",
    "- Goodness of fit\n",
    "    - $R^2$\n",
    "        - explained or 'accounted for' variance\n",
    "        - amount of variance captured by the model\n",
    "    - bad to use for telling how good a model is at predicting\n",
    "        - $R^2$ will always go up as you add more variables\n",
    "        - cross validation is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Tips\n",
    "- When in doubt, take the log of predictor variables\n",
    "- Avoid collinearity\n",
    "    - using independent variables that are highly correlated with each other\n",
    "    - tends to inflate regression coefficient, create high variances in estimates, and high instability\n",
    "    - variance inflation factor helps to measure collinearity (1-2 very little to no collin. while over 20 is extreme collin.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- Basics\n",
    "    - good for predicting binary outcomes\n",
    "    - think of the logistic s curve, the bottom is the first outcome with a value of 0, and the second outcome is the top of the curve with an outcome of 1\n",
    "    - provides a framework for considering/controlling for variables \n",
    "- Odds ratio\n",
    "    - probability of an outcome is $p$\n",
    "    - odds of the outcome are $p/(1-p)$\n",
    "        - odds = 10/1\n",
    "        - probability: 10 = p/(1-p) -> 10 - 10p = p -> 10 = 11p -> p = 10/11 = ~0.91\n",
    "        - if p is very small, 1-p is about 0 and odds are about equal to prob, but only in this case\n",
    "    - odds ratio = $\\frac{(p_1/(1-p_1))}{(p_2/(1-p_2))}$\n",
    "        - where $p_1$ is the probability of outcome 1 and $p_2$ is the probability of outcome 2\n",
    "- Strategy\n",
    "    - use Maximum Likelihood Estimation (MLE) to estimate parameters from the data\n",
    "    - define variables\n",
    "        - y is 0 or 1 (false or true) for the variable you are trying to predict\n",
    "        - can have multiple x variables\n",
    "            - can be binary 0/1 (presence/absence, gender, etc.)\n",
    "            - can be continuous (age, etc.)\n",
    "    - want the probability of Y given your X variables\n",
    "        - $p = P(Y=1|X_1, X_2, ..., X_k)$\n",
    "    - logistic regression model (logit)\n",
    "        - $logit(p) = ln (\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + ... + \\beta_k X_k$\n",
    "    - once the $\\beta$ values are calculated above, you can plugin values for your $X$ variables to calculate probabilities and make predictions\n",
    "    - interpret results (adjusted odds ratio)\n",
    "        - Example:\n",
    "        - model calculates a $\\beta$ value of 1.89 for a binary X variable\n",
    "        - plug into equations and simplify to get an 'adjusted odds ratio' = $e^{\\beta_k} = ~ 6.3$\n",
    "            - (raise e to the $\\beta$ power) to calculate addjusted odds ratio\n",
    "        - the '1' state has increased odds of 6.3 compared to the '0' state\n",
    "    - model formula\n",
    "        > $p = \\dfrac{1}{1 + e^{-cx}}$ or $p = \\dfrac{1}{1 + e^{-(b_0 + b_1x_1 ... b_nx_n)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality\n",
    "- As the number of dimensions increases, the amount of 'space' in the dimension increases rapidly, while the probability space can't keep up\n",
    "    - this results in an exponential decrease in probability with dimension\n",
    "    - i.e. a circle tangent to a square takes up 0.79 of the volume of the dimension space, while a sphere tangent to a cube takes up just 0.52 of the volume of the dimension space\n",
    "        - by dimension 6 this is just 0.08 and 10 is 0.002\n",
    "- Other descriptions\n",
    "    - overfitting a model\n",
    "    - harder to cluster data, individual points seem like their own cluster (overfitting)\n",
    "    - model parameters can reach 100's or 1000's and even have more parameters than data points\n",
    "    - **most statistics were designed to describe a central tendency (the middle)**\n",
    "        - the numbers above are the random chance of a point in that space being 'central'\n",
    "        - at 100 dimensions this goes down to $1.87 * 10^{-70}$\n",
    "    - in high dimensions, there is often no choice but extrapolation\n",
    "        - the space is so sparse that you must look outside the estimate to predict (finding close neighbor points)\n",
    "- Interpolation is better than Extrapolation in many cases (straight line not always the best answer)\n",
    "    - interpolation -> new (predicted) data points are within the range of observed points\n",
    "    - exprapolation -> new (predicted) data points are beyond the range of observed points\n",
    "- Blessing, not a curse?\n",
    "    - Gelman, uses a hierarchy to give weights to the variables, and this helps to give order to the model, supposedly aggregating neighbors better within the predictive space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction for Classification Systems\n",
    "- Goal: \n",
    "    - Project the high dimension data into a lower dimension subspace that best fits the data\n",
    "        - reduce the dimensions, while keeping the same distance between points observed in the higher dimensions\n",
    "        - looking for the 'intrinsic' dimensionality\n",
    "            - data that are in 2d but basically follow a linear pattern are intrinsically 1d\n",
    "            - data that are in 3d but basically fall around the same plane are intrinsically 2d, etc.\n",
    "        - many techniques project the data into a linear plane\n",
    "- How: PCA (principal components analysis)\n",
    "    - uses\n",
    "        - dimensionality reduction\n",
    "        - compression\n",
    "        - visualization\n",
    "    - what it does\n",
    "        - pc1: minimizes the orthogonal distances\n",
    "        - pc1: represents the direction of maximum variance (or max spread of the data points)\n",
    "        - would create a 1d line for 2d data\n",
    "    - how it differs from linear regression\n",
    "        - linear regression does not rotate the original axis\n",
    "            - residuals are measured as $|y_{obs} - y_{pred}|$ (vertical differences)\n",
    "        - pca uses best fit based on orthogonal projection of the data\n",
    "            - not diff in y values, but closest distance between the point and the line (perpendicular to the line)\n",
    "        - linear regression residual values are essentially the hypoteneuse of a triangle\n",
    "        - pca residual values are one side of that same triangle\n",
    "        - therefore, the resulting best fit 'line' is different\n",
    "    - algorithm\n",
    "        - subtract the mean from each data point (center the data around the origin)\n",
    "        - (typically) scale each dimension by its variance\n",
    "            - depends on the problem and its application\n",
    "            - divide each dimension parameter (x, y, z...) by the variance in that direction\n",
    "            - helps pay less attention to magnitude of dimensions\n",
    "        - compute covariance matrix S\n",
    "            - $S = \\frac{1}{N}X^TX$\n",
    "            - original matrix X transposed times X\n",
    "            - captures the spread in each dimension and all of the cross products\n",
    "        - compute the k largest eigenvectors from S\n",
    "            - these are the principal components\n",
    "        - possible to add weighting by:\n",
    "            - having fewer samples of lower weighted features\n",
    "            - using other algorithms that allow for weighted features\n",
    "    - SVD (singular value decomposition)\n",
    "        - the way that most people compute PCA\n",
    "        - much more robust than doing by hand like above\n",
    "    - Screeplot: plot eigenvectors along the x axis and variance explained on the y axis\n",
    "        - enough to calculate 80-90% of the variance is usually good\n",
    "            - or look for where there are diminishing returns by adding more pc's\n",
    "        - there is a simple formula to calculate the variance that each eigenvector explains\n",
    "- Intrinsic Dimensionality\n",
    "    - is the number of pc's you need/use\n",
    "    - effectively reduces the dimensionality of the data\n",
    "- Project your data points onto that/those principal components\n",
    "    - for visualization purposes, you can pick the first two pc's to create a 2d plot of your points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional Scaling (MDS)\n",
    "- Similar in function to PCA with a different goal\n",
    "    - goal: find a set of points whose pairwise distances match a given distance matrix\n",
    "    - input is a matrix of distance vectors between points rather than the raw data\n",
    "        - decide distance metric, compute all of the pairwise distances, and assemble in distance matrix\n",
    "    - classical MDS methods\n",
    "        - given n x n matrix of pairwise distances between each point\n",
    "        - compute n x k matrix X with coordinates of distances with some linear algebra magic\n",
    "        - perform PCA on this matrix X\n",
    "    - more trying to preserve the distances between points rather than minimize the orthogonal distances between the data and the projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with 'Wide' Data\n",
    "- wide data = lots of variables, whereas tall data has lots of observations\n",
    "    - table shape\n",
    "- Ridge Regression\n",
    "    - an extension of linear regression\n",
    "    - takes the sum of the squared residuals between observed/model + the sum of squared $\\beta$'s times $\\lambda$ which is a tuning parameter \n",
    "        - (penalty factor)\n",
    "        - imposes a penalty to the model related to the number of variables used (to promote dimensionality reduction)\n",
    "    - there are other types of regression that also punish too high dimensionality starting with OLS, then adding a penalty factor\n",
    "- Shrinkage Estimation\n",
    "    - not out of Seinfeld\n",
    "    - **Stein's Paradox**\n",
    "        - use total MSE to calculate performance (standard loss function)\n",
    "        - try to estimate 3 or more means for predicting 3 or more independent y values (dependent variables)\n",
    "        - prediction estimates tend to shrink towards a grand mean (sort of like regression towards the mean)\n",
    "        - makes the most sense when 'independence' may not be complete (i.e. many different baseball players' avgs.)\n",
    "- LASSO and Sparsity\n",
    "    - helps to induce sparsity, which reduces dimensions\n",
    "        - will reduce hierarchy of $\\beta$'s to 0 (reduces the number of variables that matter)\n",
    "    - like Ridge regression, but penalty term is sum of $\\lambda * abs(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Assigning labels to data\n",
    "- Examples: \n",
    "    - Email -> spam or not spam\n",
    "    - Text -> which language is it\n",
    "    - Images -> classify images as a type or search for features\n",
    "- Greatly enhanced by machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Strategy\n",
    "- Input:\n",
    "    - training dataset of N data points, each labeled as one of K different classes\n",
    "- Learn:\n",
    "    - use the training dataset to learn about the classes\n",
    "- Evaluate:\n",
    "    - predict the labels for a test set and compare to the true labels (ground truthing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Explanation\n",
    "- k number 'features' are used to essentially plot the data in a k dimensional space\n",
    "- we are looking for the decision boundary(ies) between the different classes\n",
    "- data points are typically called 'x' and the labels called 'y'\n",
    "- depends heavily on feature selection\n",
    "    - crappy features will result in overlap and no real decision boundary\n",
    "    - good features can result in obvious decision boundaries\n",
    "    - cross validation helps with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors Classification\n",
    "- Properties\n",
    "    - Training is fast\n",
    "    - Prediction is slow\n",
    "        - must keep all data points, and compare new data point to all data points\n",
    "- Simplest way to classify\n",
    "    - 'majority voting' kind of deal\n",
    "    - look at the categories of the nearest datapoints: assign predicted categories based on the known categories that are closest in feature values to the prediction data point\n",
    "    - decisions are impacted on the 'size' of the neighborhood\n",
    "        - evaluate the performance of the classifier to pick the neighborhood size\n",
    "        - this is usually not a set size, but instead the value 'k' which is the number of neighbors to 'poll' (typically an odd number so no ties)\n",
    "    - depends heavily on 'k' or the number of neighbors to use in classification\n",
    "        - larger k means less variance but more potential bias\n",
    "        - very low k results in rough decision boundary\n",
    "        - always use an odd k\n",
    "- 1 nearest neighbor properties\n",
    "    - rough decision boundary\n",
    "    - will draw islands around points that are the 'wrong category' in another category's space\n",
    "        - no bueno when this happens\n",
    "    - however, it is simple and often quite good for well separated, low dimension data (few features)\n",
    "        - especially when there are no 'island' points in the 'wrong spot'\n",
    "    - complexity\n",
    "        - complexity of adding N points to the training set is order 1\n",
    "            - just add the additional data points/labels (N more calculations)\n",
    "        - complexity of new test data M number of points\n",
    "            - complexity is M x N, must go through every training point for each new test point\n",
    "        - weird tradeoff\n",
    "            - ideally you spend your time during training, so testing is quick, but this version is opposite that\n",
    "        - error on training set\n",
    "            - 0 because we always get the nearest neighbor\n",
    "        - variance/bias tradeoff\n",
    "            - variance is quite high (adding new training data will affect the decision boundary each time)\n",
    "            - bias is very low\n",
    "- k-NN properties (using k nearest neighbors)\n",
    "    - get rid of islands by increasing k above 1\n",
    "    - if k is too large, decision boundary can become too smooth\n",
    "        - leads to lower variance and higher bias\n",
    "- Choosing the ideal value for k (more info below)\n",
    "    - k, the 'distance' metric, and the 'majority voting' method are hyperparameters\n",
    "        - hyperparameter = a parameter whose value is set before training begins\n",
    "    - how do we measure the 'distance' between points?\n",
    "        - could be euclidian, but this makes less sense for increased dimensions, and there are other options\n",
    "    - how do you decide 'majority voting'\n",
    "        - usually probabilistically, but there are other methods that might improve the model\n",
    "    - Usually you would divide the data 60% train, 20% validation, 20% test\n",
    "        - use the 20% validation to help choose the hyperparameters\n",
    "    - Cross Validation\n",
    "        - optimize the three hyperparameters mentioned above\n",
    "        - train on the training data, test on the testing data, choose the k with the lowest test error\n",
    "        - how do you choose the number of folds? intuition! but 5 and 10 are typical numbers\n",
    "            - the size of your dataset heavily influences this, and sometimes 3 fold is all you can do\n",
    "            - **30%** of the data is typically used as test data (try to select randomly)\n",
    "                - I've also seen as little as 15% test data, depending on size of the dataset\n",
    "            1. Training/validating\n",
    "                - do not want to use entire training (labeled) dataset at once, because you can only optimize it once\n",
    "                - training multiple times will help choose the correct k that will generalize to other test data\n",
    "                - best to split the training data into smaller sets (folds)\n",
    "                - use the last of those training folds (training data) to estimate the hyperparameters, and this actually becomes a validation fold that uses 'validation data'\n",
    "                - example: you would separate the training data into five folds, four with training data, and one with validation data (order of the folds not important)\n",
    "                - finish by taking the average of the hyperparameters (or the set that is the most optimal) from each fold/validation, then apply to the test data\n",
    "            2. Cross Validation\n",
    "                - pick a fold to be the validation fold, with all other folds as training folds\n",
    "                    - used as 'test data for the hyperparameters' essentially\n",
    "                - repeat, but change the validation fold, using all other folds as training folds\n",
    "                    - do this until every fold has been used as the validation fold once\n",
    "                - example: 5 'training' folds, would do five different iterations, using each fold as the validation fold once, generating 5 sets of hyperparameters\n",
    "                - average the parameters with the best performance on the validation data (or pick if that makes more sense, but probably usually avg)\n",
    "                - test data is NOT used to determine the parameters and is used only at the very end to evaluate the model\n",
    "                    - results are only able to be generalized if evaluated in this way on test data\n",
    "            - CIFAR-10 dataset is a good example using color images to classify into categories\n",
    "                - link:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Example Dealing with high dimensional data (like images)\n",
    "- Turn the data into vectors\n",
    "    - image example using CIFAR-10 dataset\n",
    "        - each color image is 32 x 32 pixels\n",
    "        - each color image is a function of rgb\n",
    "        - each image is a vector in 32 x 32 x 3 = 3072 dimension space (unrolling/unpacking the matrix)\n",
    "- $L_1$ distance aka 'Manhattan Distance'\n",
    "    - The 'distance' between points is the absolute difference in their pixel values\n",
    "        - sum the absolute differences at each pixel location\n",
    "            - use the 3072 vector and add up 3072 numbers, where each number is the difference between two images\n",
    "            - `sum from 0 to i of (abs(diff[i] = img1[i] - img2[i]))` where i = n-1 (0 indexed)\n",
    "- $L_2$ distance aka 'Euclidian Distance'\n",
    "    - shortest distance between points plotted in a plane\n",
    "        - the square root of the sum of the differences squared\n",
    "            - `sqrt( sum from 0 to i of( ( img1[i] - img2[i] )^2 ) ) )\n",
    "            - could apply this to the image vectors above\n",
    "- More general Lp norms\n",
    "    - weird formulas, less commonly used than $L_1$ and $L_2$ distances\n",
    "- In these examples, the features are the pixels and the model doesn't work so well\n",
    "    - turns out the this doesn't work so well, and the accuracy is less than about 40% using the pixel differences as the feature\n",
    "    - the pixel method is very simple and doesn't take into account rotation, zoom, lighting, different colors of objects in the same class, etc.\n",
    "    - neural network architectures work much better (~95% accuracy) and use more features\n",
    "        - these use sifting and pick certain areas, account for rotation, perspective, lighting, etc.\n",
    "        - moral of the story is that **feature selection is very important**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Choosing Ideal k\n",
    "- Plot the results of many different tests using your fold choice (5 fold, etc.)\n",
    "    - x var is the value of k\n",
    "    - y var is the accuracy of the model (right predictions by classifier / right predictions of the ground truth)\n",
    "    - if using 5 fold validation, for each value of k, you will get 5 values as results\n",
    "        - plot line graph using error bars through the mean of the 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "- For binary classification, this is a 2 x 2 matrix\n",
    "    - columns are prediction 0, prediction 1\n",
    "    - rows are actually 0, actually 1\n",
    "    - values are true/false\n",
    "    - shows left to right, top to bottom\n",
    "        - true positive, false negative\n",
    "        - false positive, true negative\n",
    "        - record numbers there (not percentages)\n",
    "    - accuracy = sum of diagonal (true positive + true negative) / sum of entire matrix\n",
    "        - can extend for more than two options\n",
    "    - precision = true positives / true positives + false positives\n",
    "        - high precision means low false positive rate\n",
    "        - if I take a positive predict, what is the chance it actually is positive?\n",
    "    - recall = true positives / true positives + false negatives\n",
    "        - aka sensitivity, hit rate\n",
    "        - high recall means predicted most positives correctly\n",
    "        - if I take a random positive sample, what is the chance of a correct prediction?\n",
    "    - F1 score = 2 * ( precision * recall / precision + recall ) \n",
    "        - aka harmonic mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "- Hyperparameters are parameters chosen before fitting the model\n",
    "    - k in k-NN\n",
    "    - alpha/lambda in Ridge/Lasso regression\n",
    "- Strategy\n",
    "    - try a bunch of different values for the hyperparameters\n",
    "    - fit all of them separately\n",
    "    - evaluate the performance of all of them\n",
    "    - choose the best performing one\n",
    "    - essential to use cross validation for HT (or use a validation set)\n",
    "        - otherwise, you risk overfitting to the test set and losing generalization\n",
    "- Implementation \n",
    "    - choose a grid of hyperparameters to test (all combinations within a certain meaningful range of each)\n",
    "    - perform k-fold validation for each point in the grid\n",
    "    - choose the value or combination of values for the hyperparameter(s) that perform the best\n",
    "    - GridSearchCV in scikit learn can do this\n",
    "    - RandomizedSearchCV does a similar process, but doesn't test all combinations and uses less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "- Popular classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic info\n",
    "    - widely used for all sorts of classification problems\n",
    "    - some people say it's the best off-the-shelf classifier\n",
    "    - kind of weird because it focuses on the rare data points (ones on the edge of the cluster)\n",
    "    - only uses the support vectors for prediction, so faster than k-NN for predicting\n",
    "- Characteristics of SVM\n",
    "    - When using two classes y values for two classes are -1, and 1: this makes the math easier\n",
    "    - Hyperplane separates the classes\n",
    "        - $w^Tx + b = 0$\n",
    "        - positive results are evaluated as the +1 class, negative results are the -1 class\n",
    "    - w: weight vector -> orientation of the hyperplane that separates the classes (like a decision boundary)\n",
    "        - hyperplane can be a straight line for two features\n",
    "        - hyperplane passes through the origin\n",
    "        - w changes the orientation (angle) of the hyperplane\n",
    "    - b: bias -> lets you 'shift' the hyperplane around, so it doesn't pass through the origin\n",
    "        - b moves the hyperplane off the origin\n",
    "    - goal of SVM\n",
    "        - initialize w & b, then 'wiggle' that hyperplane to optimize it for class predictions\n",
    "            - uses **Maximum Margin Classification**\n",
    "                - make the hyperplane the further distance from all points as possible (largest rectangle box around the hyperplane)\n",
    "                - these closest points define the 'support vector' in SVM that helps draw the hyperplane\n",
    "                    - still need to collect and train on a large amount of data, because the support vector set are going to be the rare observations, furthest away from the mean of that particular class\n",
    "                    - $\\gamma$ 'gamma' is the width of the maximum margin\n",
    "                - $x_\\perp$ = the distance from a point x on the max margin to the hyperplane\n",
    "                    - $x_\\perp = x - \\gamma \\frac{w}{||w||}$ (the direction component of vector w)\n",
    "                        - $x_\\perp$ is on the hyperplane\n",
    "                    - if x is on the maximum margin, then $\\gamma = x - x_\\perp$\n",
    "                    - leads to $w^Tx_{\\perp}^{(i)} + b = 0$\n",
    "                    > and $\\gamma^{(i)} = y^i(\\dfrac{w^Tx^{(i)} + b}{||w||})$\n",
    "                - trying to optimize the largest $\\gamma$ (gamma is width of the margin)\n",
    "                - slack variables\n",
    "                    - training points that fall on the 'wrong side' of the decision boundary\n",
    "                    - distance is measured to the margin on the 'correct side' $\\xi$\n",
    "                        - if a yellow point is on the blue side, measure yellow point to yellow side margin\n",
    "                    - leads to an equation not listed here where coeffiecient $C$ multiplies by the sum of the slack variables\n",
    "                        - large $C$ means that the model will care a lot about minimizing the value of the slack variables (large C means not a lot of slack allowed)\n",
    "                        - lower $C$ can lead to a more robust model at the expense of some misclassification\n",
    "        - once w and b are defined, prediction is easy\n",
    "            - +1 cat -> $w^Tx + b > 0$ and -1 cat -> $w^Tx + b < 0$\n",
    "        - no longer need training data\n",
    "            - just evaluate new datapoints based on the hyperplane equation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron\n",
    "- Used to calculate the hyperplane equation\n",
    "    - structure is similar to a neuron\n",
    "    - you have n features, there are n 'dendrites' of $w_n * x_n$ then add $b$ and sum those\n",
    "        - the output is the threshold for the 'firing' of the 'neuron'\n",
    "    - no fewer than 60,000 training data points suggested for training a neural network in this way\n",
    "- Basis of deep learning models\n",
    "    - all artificial neural networks (ANN) are intrinsically about\n",
    "- Hyperparameters and extra dimensions\n",
    "    - XOR problem\n",
    "        - can't draw a straight line hyperplane between points 0,0:1,1 are blue: 0,1:1,0 are yellow\n",
    "            - box shape with alternating colors\n",
    "        - basic solutions:\n",
    "            - add a third dimension that will separate the points and use a plane to separate them\n",
    "                - only add what is necessary to separate the classes\n",
    "                - running PCA on this will keep necessary dimensions while reducing 'noisy' or not needed dimensions\n",
    "            - include $x^2$ in the model\n",
    "                - does not add any additional information or dimensions, but it will 'move' the datapoints such that a separating hyperplane can be drawn\n",
    "                - this allows the SVM to essentially have non-linear decision surfaces\n",
    "            - these are essentially the same solution\n",
    "    - Use Kernel Trick for SVM's as Generic Solution\n",
    "        - $x = (x_1, x_2)$ (2d point)\n",
    "        - $\\phi(x) = (1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2)$\n",
    "            - quadratic expansion of the 2d point\n",
    "        - this gets unruly at higher dimensions, so we are paying a cost for adding the dimensions to get a linear hyperplane, but we don't actually hae to do that, there is an easier way below to get the dot product of x and z\n",
    "            - for polynomial kernels\n",
    "            > $K(x,z) = \\phi(x) \\cdot \\phi(z) = (1 + x \\cdot z)^s$\n",
    "            - Radial Basis Function (RBF)\n",
    "            > $K(x,z) = exp(-\\gamma(x-z)^2)$\n",
    "        - tuning\n",
    "            - polynomial kernel, tune s (degree of the polynomial)\n",
    "            - RBF, tune $\\gamma$\n",
    "                - can go up to an infinite number of dimensions\n",
    "        - Kernel Trick Summary\n",
    "            - arbitrary many dimensions\n",
    "            - little computational cost\n",
    "            - maximal margin helps with curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Application\n",
    "- Divide data\n",
    "- Cross validate\n",
    "    - number of components for PCA\n",
    "    - which kernel to use (polynomial or RBF)\n",
    "    - tune hyperparameters (use exponential steps `[10^-1, 10^0, 10^1]` of `[10^-2, 10^0, 10^2]`\n",
    "        - polynomial\n",
    "            - s (degree of the polynomial)\n",
    "        - RBF\n",
    "            - gamma\n",
    "        - C (slack variable parameter)\n",
    "            - large C means less slack (leads to overfitting in some cases)\n",
    "        - always tune gamma/C jointly together because they greatly affect the model's fit (over/under fitting)\n",
    "- Tips and Tricks\n",
    "    - normalize or otherwise scale your data first! (mean = 0, std = 1)\n",
    "        - check if library normalizes by default\n",
    "        - train & test data should both be normalized, but should be done separately, so mean/std are not influenced\n",
    "    - map y to (-1, 1) or (0, 1), I think (-1, 1) is better\n",
    "    - use RBF kernel by default unless you have another reason not to\n",
    "    - use exponential sequences for gamma/C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Differences Between k-NN and SVM\n",
    "- predictions\n",
    "    - k-NN keeps/uses all the training data during predictions\n",
    "    - SVM only worries about the support vectors\n",
    "- hyperparameters\n",
    "    - k-NN only tunes k\n",
    "    - SVM has C, which kernel, then kernel parameter (gamma or s/degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing\n",
    "- as with k-NN, can plot degrees of freedom (# of params - 1) vs error\n",
    "    - plot both training error and testing (validation) error with different dof\n",
    "    - optimum is where testing (validation) error is the lowest\n",
    "- error metrics\n",
    "    - positive/negative matrix (similar to confusion matrix) tp, tn, fp, fn\n",
    "        - true positive rate = $tpr = \\frac{tp}{tp + fn}$\n",
    "        - false positive rate = $fpr = \\frac{fp}{fp + tn}$\n",
    "        - Receiver Operating Characteristics: plot fpr(x) vs. tpr(y)\n",
    "            - want to be in top left corner (100% tpr, 0% fpr) or as close as possible\n",
    "            - diagnoal line 0,0 to 1,1 is the 50/50 line, coin flip chance (worst possible case)\n",
    "            - below this line is even worse!, though if it is way below the line, just flip the labels, because the model actually does have predictive power!\n",
    "            - plot entire line varying over the parameter\n",
    "                - however, only display the AUC (area under the curve) value\n",
    "            - plot training data and testing data\n",
    "        - Recall/precision curves (good for imbalanced data: i.e. only 20% positives in the the data)\n",
    "            - recall = $\\frac{tp}{tp + fn}$ (same as true positive rate)\n",
    "            - precision = $\\frac{tp}{tp + fp}$\n",
    "            - plot recall(x) vs. precision(y)\n",
    "            - want to be in upper-right corner (high recall/high precision)\n",
    "                - can actually choose whether or not recall or precision is more important and choose accordingly\n",
    "        - F-measure (summary measure for precision/recall curves (analogous to AUC)\n",
    "            - weighted average of precision and recall (harmonic mean)\n",
    "            - $F_{\\beta} = \\dfrac{(\\beta^2 + 1)\\cdot P \\cdot R}{\\beta^2 \\cdot P + R}$\n",
    "            - usual case $\\beta = 1$\n",
    "            - increasing $\\beta$ allocates weight to recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM for Multiple Classes\n",
    "- 1 vs. all (slower training)\n",
    "    - idea\n",
    "        - single out one class, and combine all other classes together for the model\n",
    "        - do for each class individually\n",
    "    - implementation\n",
    "        - train n classifiers for n classes\n",
    "        - use 1 (positive class) for the single class\n",
    "        - let all classifers predict, take classification with greatest margin as the answer\n",
    "- 1 vs 1 (faster training)\n",
    "    - idea\n",
    "        - take only two classes at a time\n",
    "        - permute each combination of classes\n",
    "    - implementation\n",
    "        - train $n(n-1)/2$ classifiers\n",
    "        - let all classifiers you trained predict, then take majority vote\n",
    "- evaluation\n",
    "    - confusion matrix\n",
    "        - predicted label on x, true label on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "- Basics\n",
    "    - start with a question, answer determines which branch you follow\n",
    "    - this may lead to an answer or to another question\n",
    "    - the decisions are intuitive and easy from which to derive meaning (understanding/interpreting)\n",
    "        - the w and b from SVM don't tell as much\n",
    "        - whereas MMSE value over 6 may provide more practical information\n",
    "    - fast training\n",
    "    - fast prediction\n",
    "    - are not that popular\n",
    "        - they tend to overfit\n",
    "        - tree pruning can help some with this\n",
    "- Ideas\n",
    "    - look at one feature at a time\n",
    "        - pick a threshold and evaluate a simple Y/N or T/F is $x_1 > \\theta_1$\n",
    "            - make a decision then continue through the tree\n",
    "        - pick another threshold on another feature $x_2 > \\theta_2$\n",
    "            - make a decision then continue through the tree\n",
    "    - once finished, the 'feature space' where the points dwell is divided into cells\n",
    "        - entire classes are assigned to the 'cells'\n",
    "    - drawbacks\n",
    "        - by analyzing one feature at a time (greater than/less than), you can only draw straight line splits to create the 'cells'\n",
    "            - would take a lot of decisions (splits) to create a diagnonal line\n",
    "            - to 'smooth' out the line and not have it too pixelated requires even more splits (decisions)\n",
    "    - benefits\n",
    "        - because only 1 feature at a time, you don't need to normalize or scale the data first\n",
    "        - multi-class features are very easy with decision trees\n",
    "            - unlike SVM\n",
    "        - very fast\n",
    "        - very intuitive/interpretable\n",
    "    - most overfit model will have one cell for every training data point\n",
    "- Disadvantages\n",
    "    - sensitive to small changes in data\n",
    "    - prone to overfitting\n",
    "    - only axis aligned splits (one parameter at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DT vs. SVM\n",
    "- DT is better at\n",
    "    - multiple classes\n",
    "    - mixed data types (no standardization needed)\n",
    "    - handling missing values\n",
    "    - more robust with outliers\n",
    "    - insensitive to monotone transformation of inputs\n",
    "    - scales well (large N)\n",
    "    - ability to deal with irrelevant inputs\n",
    "    - interpretability\n",
    "- SVM is better at\n",
    "    - ability to extract linear combinations of features\n",
    "    - **predictive power** generally makes SVM better, though SVD (singular value decomposition) can help DT's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Training\n",
    "- Must learn tree structure\n",
    "    - which feature to query?\n",
    "    - which threshold to use?\n",
    "    - want to optimize 'node purity'\n",
    "        - creating cells that only contain one category\n",
    "        - ok to have 4 cells that are all red, this is the prediction space for red\n",
    "            - one way to have a non-linear decision boundary\n",
    "    - Gini impurity\n",
    "        - 0.5 is bad (worse case scenario)\n",
    "            - high Gini index\n",
    "            - high entropy\n",
    "            - high missclassification error\n",
    "        - expected error if randomly choosing a sample and predict the class of the entire node base on the sample\n",
    "        - probablility of making an error for each class is prob of that class * prob of an error (sum of the probs of each other class)\n",
    "        - Gini impurity is the sum of all those probabilities of making an error\n",
    "        - for each n (each n gets to be the main n, while multiplying by the probs of the other n's then sum it up)\n",
    "        - Generalizing (sum of probs of true class * wrong prediction)\n",
    "            - $C$ number of classes\n",
    "            - $N$ number of datapoints\n",
    "            - $N_i$ number of datapoints in class i\n",
    "            - $I_G$ purity of a node\n",
    "            > $I_G = $$\\sum_{n=1}^{C} \\frac{N_i}{N} (1-\\frac{N_i}{N})$\n",
    "    - optimizing the tree\n",
    "        - should I make a split here?\n",
    "            - Gini impurity of parent node vs. Gini impurity of child nodes\n",
    "        - misclassification\n",
    "            - is predicted label the same as the assigned label? divided by the number of points\n",
    "            - problem: this is not differentiable\n",
    "            - can be the same between two different decisions for a split (though one decision produces a pure node and the other does not)\n",
    "                - Gini index gives more weight to the split that produces a pure node\n",
    "                - ex. split 400/200, 200/0    vs. 300/100, 100/300\n",
    "                    - both have 0.25 misclassification\n",
    "                    - Gini gain for left side is 0.166 and the right is 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees Process (pseudocode)\n",
    "- Check if finished\n",
    "    - do all cells only have 1 class in them? (one choice for when to stop)\n",
    "- For each feature $x_i$\n",
    "    - calculate gain from splitting on $x_i$\n",
    "    - let $x_best$ be the feature with the highest gain\n",
    "- Create a decision node that splits on $x_best$\n",
    "    - repeat on the sub-nodes that are left\n",
    "- When to stop?\n",
    "    - nodes only contain one class\n",
    "    - nodes contain less than x data points\n",
    "    - max depth is reached\n",
    "    - node purity is sufficient (above a threshold)\n",
    "    - you start to overfit (cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning\n",
    "- Helps with overfitting and too many decisions\n",
    "    - work backwards, removing decisions\n",
    "    - creates more impure cells\n",
    "        - use majority voting in those cells for classification\n",
    "        - can also use probabilities in these cells for classification (ratio of the points from each class)\n",
    "- Pruning and Complexity\n",
    "    - bias/variance tradeoff (over/under fitting)\n",
    "    - plot complexity (number of nodes) vs. prediction error\n",
    "    - optimize where test (validation) error is the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods and Random Forests\n",
    "- **Ensemble Methods**\n",
    "    - Can use these methods for other classifiers (not helpful for linear regression)\n",
    "    - single DT's don't perform well but are fast\n",
    "    - can use multiple trees\n",
    "    - just need to ensure they don't all learn the same way\n",
    "        - bootstrap the data to simulate collecting more data\n",
    "        - will create many slightly different trees you can use on new data, taking the average of all the results\n",
    "        - you cannot use the bootstrap samples as cross validation, because there is no 'unseen' data\n",
    "            - the bootstrap samples will contain essentially the same data points, with some extra randomness to them\n",
    "            - probability of choosing N data point again is about 0.632 (very high)\n",
    "            - still need 'unseen' data for cross validation (validation folds)\n",
    "    - **Bagging** = bootstrap aggregating\n",
    "        - bootstrap your sample dataset\n",
    "            - before bs-ing first set aside test data\n",
    "        - train a classifier on each set\n",
    "        - predict for new data points\n",
    "            - run data through all generated classifiers and take the average\n",
    "        - does a pretty good job of generalizing and catching the variance without introducing too much bias\n",
    "            - let's you create lots of high variance trees, then taking the average will smooth out the decision boundary\n",
    "        - two ways to measure performance of bagging\n",
    "            - consenus: (mode of the total number of predictions) - voting\n",
    "                - can add weights to specific models/learners\n",
    "            - probabilitiy\n",
    "        - **benefits**\n",
    "            - reduces overfitting (variance, very wiggly decision boundaries)\n",
    "            - normally only one type of classifier (DT, SVM, k-NN): can use with them all\n",
    "                - however, often used with DT's because they have high variance and they are fast, so can bootstrap quickly\n",
    "            - does NOT help with linear models\n",
    "            - easy to parallelize (MLlib included with Spark)\n",
    "    - **Random Forest**\n",
    "        - builds on bagging\n",
    "            - **main difference** between RF and bagging is using subset of features each time\n",
    "                - often sqrt(M) where M is the total number of features\n",
    "                - select this subset randomly\n",
    "                - introduces more randomization to the model\n",
    "                - makes model better at generalizing and reduces variance (smoother boundary)\n",
    "                - loses a little interpretability (on how classification was done, thresholds)\n",
    "            - each tree from a bs sample (remember to only bs the training data set)\n",
    "            - no pruning, keep bagged trees as is\n",
    "            - hyperparameter tuning is more intuitive and outlined than for neural nets\n",
    "                - number of trees, number of features\n",
    "    - **Boosting**: very popular\n",
    "        - like bagging except:\n",
    "            - weak learners evolve over time\n",
    "                - training sets are not independently chosen (like in bootstrap)\n",
    "            - votes are weighted\n",
    "            - better than bagging in some cases\n",
    "                - though not necessarily better than random forests\n",
    "            - more prone to overfitting than random forest\n",
    "        - **process**\n",
    "            - after building a model (tree) the model is examined and datapoints are (re)weighted\n",
    "                - points that were misclassified are assigned a higher weight in the next tree\n",
    "                - this continues as more trees are built\n",
    "                - trees with better performance are assigned more weight\n",
    "            - tuning hyperparameters\n",
    "                - number of trees\n",
    "                - number of splits (often stumps work well: only one split)\n",
    "                - parameters controlling how weights evolve with each tree\n",
    "        - AdaBoost\n",
    "            - most successful/popular boosting algorithm\n",
    "            - pseudocode\n",
    "                - two group examples with -1 and 1 as the groups\n",
    "                - start with equal weights on the observations (datapoints)\n",
    "                - generate a total of M classifiers (G)\n",
    "                    - fit a classifier with the current weights\n",
    "                    - compute the error\n",
    "                    - reweight the observations based on the error\n",
    "                    - calculate the model weight based on the error\n",
    "                    - repeat for the next classifier for M total classifiers\n",
    "                - result is the sign of the sum of all of the models times their weight\n",
    "                    - weighted majority voting\n",
    "        - Gradient Boosting (GBM)\n",
    "- Regression Trees\n",
    "    - the decision bins are continuous rather than categorical\n",
    "    - the mean of the values assigned to the bin becomes the decision\n",
    "    - also use the weighted average of all of the trees as the outcome (weight the learners/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Methods\n",
    "- Bayes rule\n",
    "    > $P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$\n",
    "- $P(A|B)$ is the posterior (probability of A given B)\n",
    "- $P(A)$ is the prior probability of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- method for updating your belief\n",
    "    - update probabilities after observing data $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential updating\n",
    "    - posterior becomes the new prior, then do again\n",
    "    - posterior becomes the new prior ...\n",
    "- Bayes rule (likelihood version)\n",
    "    > $P(\\theta|y) = \\dfrac{P(y|\\theta)P(\\theta)}{P(y)}$\n",
    "- $\\theta$ is an unknown, while $y$ is known\n",
    "- treating the data $y$ as fixed leads to\n",
    "    > $P(\\theta|y) \\propto L(\\theta)p(\\theta)$\n",
    "    - posterior density is proportional to the likelihood function times the prior density\n",
    "    - $P(y)$ is hard to calculate, so this works well\n",
    "- works best with large sample sizes\n",
    "    - the likelihood function dominates the prior and choice of the prior isn't vital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "- Assumptions\n",
    "    - conditional independence for each value\n",
    "        - so multiply these propabilities when doing caculations (factor them)\n",
    "        - huge assumption but huge simplification in statistical/computational complexity\n",
    "            - independence assumption means that the individual probabilities are naive (independent) of each other, so the probability of all of them occuring is the product of their individual probabilities rather than some crazy calculation\n",
    "        - often unrealistic, but still useful because it reduces the complexity of the model\n",
    "        - model is wrong, though it is useful\n",
    "- Pseudocode\n",
    "    - set up full probability model - joint probabilities of all observed and unobservable quantities in the data \n",
    "    - conditioning on observed data and calculating the appropriate posterior distribution\n",
    "    - evaluate the fit of the model and the implications of the posterior distribution\n",
    "- Conjugate priors\n",
    "    - Beta/binomial is the most popular choice for the prior distribution of $\\theta$\n",
    "        - $X|p \\sim binom(n,p)$\n",
    "        - $p \\sim Beta(a,b)$\n",
    "        - $p|X = x \\sim Beta(a + x,b - n - x)$\n",
    "            - this is how you update your beliefs\n",
    "            - generate new posterior that then becomes the prior\n",
    "            - $a$ number of prior successes and $b$ the number of prior failures, $n$ is sample size\n",
    "            - $x$ are new successes, so $n-x$ are new failures\n",
    "        - the conjugacy of this says that the posterior distribution is also a beta distribution\n",
    "        - if you don't have much prior data, start by choosing $a$ and $b$ as small numbers\n",
    "            - increasing $a$ and $b$ gives more weight to your prior\n",
    "            - if sample size is reasonably large, Beta(0.7,0.5) makes no difference btw Beta(1,1)\n",
    "                - likelihood will dominate the prior\n",
    "    - Normal/normal choice for the prior distribution of $\\theta$\n",
    "        - $y|\\mu \\sim N(\\mu,\\sigma^2)$\n",
    "        - $\\mu \\sim N(\\mu_0,\\tau^2)$\n",
    "        - $y|\\mu \\sim N \\left( (1-B)y + B\\mu_0,\\dfrac{1}{\\frac{1}{\\tau^2} + \\frac{1}{\\gamma^2}} \\right)$\n",
    "        - $\\mu_0$ and $\\tau^2$ are the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
