{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# import glob\n",
    "from glob import glob\n",
    "\n",
    "# import create_engine from sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# import web scraping functions\n",
    "from urllib.request import urlretrieve, urlopen, Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Info and Commands\n",
    "#### Basic Commands\n",
    "- Checking efficiency\n",
    "    - `%%time`\n",
    "        - add this as the first line of a cell to check run time and compare efficiencies of syntaxes\n",
    "        - worth it to optimize code in many cases\n",
    "- These commands assume a dataframe name of `df`\n",
    "    - `len(df)` returns the number of rows in the `df`\n",
    "- Methods\n",
    "    - Print the 'head' or first 5 rows\n",
    "        - `print(df.head())` use the '.head()' method of a dataframe\n",
    "        - works without the `print()` call\n",
    "        - supply an optional `int` as an arg to display that number of rows\n",
    "    - Print the 'tail' or last 5 rows\n",
    "        - `print(df.tail())`\n",
    "        - works without the `print()` call\n",
    "        - accepts optional int arg just as .head() does\n",
    "    - Access the 'keys' of a dataframe\n",
    "        - `df.keys()`\n",
    "    - Access general info including number of non-null values and datatypes\n",
    "        - `df.info()`\n",
    "- Attributes\n",
    "    - Print the 'columns' of a dataframe\n",
    "        - `df.columns`\n",
    "    - Print the 'indexes' of a dataframe\n",
    "        - `df.index`\n",
    "- Combining indexing with other methods\n",
    "    - you have a dataframe that each key is associated with a dataframe (a dataframe of dataframes)\n",
    "    - `print(df['key1'].head())` will print the head of the dataframe associate with 'key1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats\n",
    "- `df.describe()`\n",
    "    - provides the following summary stats for each column\n",
    "        - count, mean, std, min, 25% (1st quartile), 50% (median), 75% (3rd quartile), max\n",
    "    - null entries are ignored for all stats\n",
    "        - counts only include non null values\n",
    "    - can supply an index `df.col.describe()` to restrict to specific columns\n",
    "        - can also supply a list of columns using bracket notation\n",
    "            - `df[['col1', 'col2']].describe()`\n",
    "        - provides different results for categories\n",
    "            - unique (num distinct entries), top (most frequent entry), freq (occurrences of top)\n",
    "- `df['col'].unique()`\n",
    "    - returns a list of unique values for the column\n",
    "    - best used with categorical data\n",
    "    - can count the number of these\n",
    "- `df.col.nunique()`\n",
    "    - returns the number of unique values\n",
    "- `df.col.value_counts()`\n",
    "    - returns each unique value along with the number of occurrences\n",
    "- `df.quantile(q)`\n",
    "    - where q is a fractional number\n",
    "        - can supply a list of such numbers, output is provided in labeled rows\n",
    "        - interquartile range (IQR)\n",
    "            - `df.quantile([0.25, 0.75])`\n",
    "    - provides the value at the specified quantile (0.25, 0.5, 0.75, etc.)\n",
    "    - command above produces output for all numerical columns\n",
    "    \n",
    "#### Cross Tab\n",
    "- `pd.crosstab(df.col1, df.col2)`\n",
    "    - 'col1' and 'col2' should be categorical data\n",
    "    - will output a frequency table with the number of occurrences where they overlap\n",
    "        - i.e. 'F' and 'White', 'M' and 'White'...\n",
    "    - the values from the first col passed will be outputted as the index, with the second as column labels\n",
    "\n",
    "#### Group By\n",
    "- Let's you run a function to get summary information\n",
    "    - `df.groupby('col_name').count()`\n",
    "        - can use other functions besides `count()`\n",
    "            - `.size()`\n",
    "                - will return the actual series, and provide count() data\n",
    "            - `.mean()`\n",
    "            - `.std()`\n",
    "            - `.sum()`\n",
    "            - `.median()`\n",
    "            - `.first()` or `.last()`\n",
    "            - `.max()` or `.min()`\n",
    "            - `.idxmax()` or `.idxmin()` returns the row id/index for the max/min value\n",
    "                - specify arg of `axis=columns` to look within a row and return the column header\n",
    "            - `.nunique()` counts the number of unique values\n",
    "            - etc.\n",
    "        - **can do multiple agg functions at once using `.agg()`**\n",
    "            - `df.groupby('col_name').agg([max, sum])`\n",
    "            - can define and pass a custom function\n",
    "                - this function must accept a series of values and return a single value\n",
    "                - pass the function as an arg, without `()`\n",
    "                - `df.groupby(['col1', 'col2'])[['value1', 'value2']].agg(custom_func)`\n",
    "            - can use different aggregation functions by passing a dictionary to `.agg`\n",
    "                - 'keys' of the dictionary are column names\n",
    "                - 'values' of the dictionary are agg functions to use\n",
    "                - `df.groupby('col_name')['value1', 'value2'].agg({'value1': 'sum', 'value2': custom_func})`\n",
    "    - will 'group' data by the values in `'col_name'` and run the summary function\n",
    "        - useful when 'grouping' by categorical data\n",
    "    - can slice a single value from your `.groupby` column\n",
    "        - `df.groupby('col_name')['value'].sum()`\n",
    "            - can supply a list within the slice brackets (need two sets of brackets)\n",
    "    - can use a multi-index for the `groupby` by passing a list as the `'col_name'` index\n",
    "        - `df.groupby(['col1', 'col2'])['value'].mean()`\n",
    "    - can grab a single column\n",
    "        - `df.groupby('col1').col2.mean()\n",
    "- Example to create a custom group by 'decade'\n",
    "    - `df` contains a 'year' column, and I want to count a number of occurrences by decade\n",
    "        - `df.groupby(df.year // 10 * 10).col1.size()`\n",
    "        - the `.size()` agg function will display my year and the result (count()) just displays the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Converting DataFrames\n",
    "\n",
    "#### Creating a Dataframe\n",
    "- From a python dictionary as `dict`\n",
    "    - `df = pd.DataFrame(dict)`\n",
    "        - keys become column names, values remain values\n",
    "        - row labels are auto generated, 0 indexed\n",
    "        - values are broadcast to fill entire column if unequal length\n",
    "            - `df = pd.DataFrame({'Names': list_of_names, 'Sex': 'M'})`\n",
    "            - every row has a value of 'M' for 'Sex' column\n",
    "- From python lists\n",
    "    - generate a list of `column_labels`\n",
    "    - generate a list of `values`, which is a list of lists\n",
    "        - the inner lists hold all the values\n",
    "        - the outer list holds variables for the inner lists\n",
    "            - must be in the correct order for the labels\n",
    "            - See example below for code\n",
    "    - use `dict(list(zip(column_labels, values)))`\n",
    "        - can break this up into two or more steps for readability\n",
    "    - use `pd.DataFrame` on the dict created above\n",
    "- From a CSV file\n",
    "    - `df = pd.read_csv('path/to/file.csv')`\n",
    "        - can set `index_col=0` if you don't want pandas to auto generate row numbers in an unnamed column\n",
    "            - or name a column as a string `index_col='date'`\n",
    "        - set `header=None` if there are no column headers\n",
    "        - see 'Working with Flat Files' section below\n",
    "        - can chain `.sort_index()` onto the `pd.read_csv().sort_index()` command\n",
    "    - Formatting CSV for input\n",
    "        - good practice to use unnamed col with row keys\n",
    "        - can let pandas generate this, or set up CSV file with no col name and row keys in first col\n",
    "- Adding a column to a dataframe\n",
    "    - `df.loc[:, 'new_column'] = values`\n",
    "        - can use calculated values, values from another column, or assign a single value for each row\n",
    "\n",
    "#### Changing Index (row labels)\n",
    "- Create a list as `list_of_indexes` with length equal to the number of rows\n",
    "    - `df.index = list_of_indexes`\n",
    "- Convert to time series index\n",
    "    - `df.index = df[time_column]`\n",
    "    - `df = df.sort_index` to then sort by that index\n",
    "    - Another way, if you have a 'Date' column as a string in your df\n",
    "        - `df.Date = pd.to_datetime(df['Date'])`\n",
    "        - `df.set_index('Date', inplace=True)`\n",
    "            - can modify the index this way without having to reassign it to the df\n",
    "    - - **Parse the date and set as index during import**\n",
    "        - set `index_col='date'` and `parse_dates=True` or just set 'index_col' to your date col if not parsing\n",
    "- Rename your index column\n",
    "    - `df.index.name = 'index_name'`\n",
    "    - Delete index name\n",
    "        - `df.index.name = None`\n",
    "- Rename the name for your columns index\n",
    "    - Your list of columns is an index that you can name\n",
    "        - `df.columns.name = 'name'`\n",
    "- Reindexing\n",
    "    - Use to create a new df from a subset of another df\n",
    "        - `new_df = old_df.reindex(columns=column_list)`\n",
    "            - where column_list is a list of columns you want in the new df\n",
    "    - Assigns a new index to a dataframe\n",
    "        - `df.reindex(list[, method=])`\n",
    "            - reindexes the dataframe using the supplied list\n",
    "        - tries to match based on the old dataframe\n",
    "            - you can supply the current indexes but in a new order\n",
    "            - pandas will sort the entire df, keeping rows together\n",
    "            - **you can even supply the index of another dataframe**\n",
    "                - `df.reindex(df2.index)`\n",
    "        - any missing values are filled with NaN by default\n",
    "            - `method='ffill'`\n",
    "                - forward fill, fills missing values with last non-null value\n",
    "            - `method='bfill'`\n",
    "                - backward fill, fills from next value backwards\n",
    "- **Hierarchal Indexing** (multiple indexes)\n",
    "    - **Set index and reset index**\n",
    "        - `df = df.set_index([col1, col2][, drop=][, append=])` supplying multiple columns to use for the index\n",
    "            - order of columns matters, put fewest values first\n",
    "                - i.e. 'col1' can have several values in 'col2' for an index\n",
    "                - like 'col1' is a date and 'col2' are times on that date\n",
    "            - `drop=True` will remove the column from the data when it adds it as an index\n",
    "                - default is False\n",
    "            - `append=True` will add the column as an index, rather than replacing the index\n",
    "        - `df = df.reset_index('col2')`\n",
    "            - would remove 'col2' from the index and turn it back into a column\n",
    "            - produces no weird effects like 'unstack' below\n",
    "        - print the index names for this type of index\n",
    "            - `print(df.index.names)` to print the list of names\n",
    "        - `df = df.sort_index()` to then sort the index\n",
    "    - **Selecting using loc** \n",
    "        - order is important for slicing\n",
    "            - supply the index col names in the correct order\n",
    "        - **Translations for selecting**\n",
    "            - `index_value1` is a value from the 1st index column\n",
    "            - `second_index_value1` is another value from the 1st index column\n",
    "            - `index_value2` is a value from the 2nd index column\n",
    "        - by supplying a tuple for your index\n",
    "            - `df.loc[(index_value1, index_value2), :]`\n",
    "                - all rows at that index\n",
    "            - `df.loc[(index_value1, index_value2), 'column_name']`\n",
    "                - returns a single value at the intersection\n",
    "            - `df.loc[(['index_value1', 'second_index_value1'], 'index_value2'), 'column_name']\n",
    "                - can also use `:` instead of `'column_name'` to select all columns\n",
    "                - returns rows with index 1 values in the supplied list and index 2 value\n",
    "            - `df.loc[('index_value1', ['index_value2', 'second_index_value2']), 'column_name']\n",
    "                - returns all rows with first index of `index_value1` and in list from index 2\n",
    "                - can limit to certain columns or select all at `column_name` location\n",
    "        - slice by an outer index only\n",
    "            - `df.loc['index_value1']`\n",
    "                - returns all rows where the outer index has the supplied value\n",
    "            - `df.loc['index_value1':'second_index_value1']`\n",
    "                - can select a range of values from an index\n",
    "        - slice by both indexes\n",
    "            - must use the `slice()` function\n",
    "                - `df.loc[(slice(None), slice('index_value2', 'seond_index_value2')), :]`\n",
    "                    - `slice(None)` will select all from the first index\n",
    "                    - `slice('index_value2', 'seond_index_value2')` will slice only those values inclusive\n",
    "                    - can also just pass a single value rather than the `slice()` function in the tuple\n",
    "    - **Unstacking a multi-index**\n",
    "        - 'U' from unstack means moving an index level 'up' to a column level\n",
    "        - Remove an index and will add an 'index level' to the columns\n",
    "            - if you 'unstack' the 'sex' index, each column will now be divided into the diff 'sex' values\n",
    "        - `df.unstack(level='index_name')`\n",
    "            - can supply the name, or the index level (0 indexed) as an int\n",
    "            - often chain `.fillna(value)` to specify how to handle NaN values\n",
    "                - sometimes '' empty string can be useful for NaN values\n",
    "        - see `reset_index()` above if 'unstack' is not producing desired effects\n",
    "        - **Stacking and unstacking** works to move columns to indexes and vice versa\n",
    "        - **Set index and reset index** works to move the data in the columns to indexes and vice versa\n",
    "    - **Stacking**\n",
    "        - can only do this when you have hierarchal columns\n",
    "            - i.e. each column is divided into 'M', and 'F'\n",
    "            - turns this level into an inner index level\n",
    "        - `df.stack(level='index_name')`\n",
    "    - **Swapping Index Levels**\n",
    "        - `df.swaplevel(index_int1, index_int2)`\n",
    "            - supply the int for the index i.e. (0,1)\n",
    "        - probably need to `df.sort_index()` after this\n",
    "\n",
    "#### Changing Column Labels\n",
    "- `df.columns = list_of_labels`\n",
    "    - the `list_of_labels` must include the correct number of labels\n",
    "    \n",
    "#### Convert Columns to Different Data Types\n",
    "- Useful to convert 'string' data types into 'categories' if only a few unique values\n",
    "- Check the number of unique values in a column\n",
    "    - `df['col_name'].unique()` \n",
    "        - displays all of the unique values in that column as a list\n",
    "- Convert the type to categorical if reasonable to do so\n",
    "    - `df['col_name'] = df['col_name'].astype('category')`\n",
    "        - this saves memory and increases processing speed for operations on the df\n",
    "\n",
    "#### Convert Dataframe to Numpy Array\n",
    "- `array = dataframe.values`\n",
    "    - these \"values\" must all be the same type\n",
    "    \n",
    "#### Sorting by a column\n",
    "- `df.sort_values('column_name'[ascending=])`\n",
    "    - default is `ascending=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Elements in a Dataframe\n",
    "#### Select rows or columns, dataframe as `df`\n",
    "- Selecting based on indexes works faster if your indexes are sorted\n",
    "    - `df = df.sort_index()` prior to running (or can save as `df_sorted` if needed)\n",
    "- Selecting values\n",
    "    - use the `.values` attribute to return the values as a numpy array\n",
    "    - can use this on a single column or selection\n",
    "        - `df_series_obj = df['column_name']`\n",
    "        - `np_array = df.series_obj.values`\n",
    "        - or simply `np_array = df['column_name'].values`\n",
    "    - select a single 'cell'\n",
    "        - `df['col_name']['row_index']` or `df.col_name['row_index']`\n",
    "- Select your index values\n",
    "    - `idx_vals = df.index.values`\n",
    "        - returns a list of the index values for each row\n",
    "- Select entire column(s) using col names\n",
    "    - `df['column_name']` or `df.column_name`\n",
    "        - returns a pandas series object, with extra info in it (not a dataframe)\n",
    "    - `df[['column_name']]`\n",
    "        - returns a dataframe object\n",
    "        - can supply multiple column names in the supplied list\n",
    "- Selecting row(s) with slicing\n",
    "    - `df[start:end:stride]`\n",
    "        - specify the index for *start* and *end*\n",
    "        - works like slicing a list\n",
    "        - returns the *start* index and stops at index before the *end* index\n",
    "        - leave *start* empty to start at the beginning\n",
    "        - leave *end* empty to slice until the end\n",
    "        - *stride* is the frequency of elements to choose (blank=1)\n",
    "- Selecting rows/cols with slicing\n",
    "    - `df[col][row]`\n",
    "        - `col`\n",
    "            - can simply be a column name\n",
    "            - can use `[start:end:stride]` syntax\n",
    "        - `row`\n",
    "            - can simply be a row name\n",
    "            - can use `[start:end:stride]` syntax\n",
    "- Selecting time series indexed rows with slicing\n",
    "    - `df.loc['datetime']`\n",
    "        - supply the datetime as a string\n",
    "        - if date and time info, can supply a date, to select all rows/times from that date\n",
    "            - can supply just the year, year-month, year-month-day\n",
    "                - will select everything that applies\n",
    "    - `df.loc['start':'end']`\n",
    "        - selects everything in the range, including the 'stop' datetime\n",
    "        \n",
    "#### **Selecting Rows and Columns**\n",
    "- `reindex` to subset a df\n",
    "    - `new_df = old_df.reindex(columns=column_list)`\n",
    "        - where `column_list` is a list of columns to include\n",
    "- loc\n",
    "    - `df.loc['row_label', 'col_label']`\n",
    "        - returns a single value\n",
    "    - `df.loc['row_label']`\n",
    "        - returns a pandas series object\n",
    "        - values are returned as a list\n",
    "    - `df.loc[['row_label']]`\n",
    "        - returns a pandas dataframe object\n",
    "        - can include multiple row labels in the list\n",
    "    - `df.loc[['row_label'], ['column_label']]`\n",
    "        - can supply a second list containing column labels to select\n",
    "    - `df.loc[:, ['column_label']]`\n",
    "        - selects all rows and the columns you specify in the second list\n",
    "    - `df.loc[rows, cols]`\n",
    "        - can use `[start:stop:stride]` for rows/cols, supply names still\n",
    "            - the `stop` column is included, as it is named\n",
    "            - returns a range from start to stop inclusive\n",
    "        - can use `:` for all rows/cols\n",
    "        - can supply a single label to select that row/col\n",
    "        - can supply a list\n",
    "    - `df.loc[(index1, index2)]`\n",
    "        - multi-index selections\n",
    "            - string indexes should go in ''\n",
    "- iloc\n",
    "    - Syntax for `row`, `column`, and `stride`\n",
    "        - `row` & `column`\n",
    "            - supply an int to access a single row/column\n",
    "                - an int for both will return a single value\n",
    "            - supply a list within `[]` to access multiple rows/columns\n",
    "            - using `:`\n",
    "                - `[start:stop]`\n",
    "                - `[:3]` start at the beginning and include 3 rows/columns (not inclusive of index 3)\n",
    "                - `[3:]` start at index 3 and include the rest of the values\n",
    "    - same as loc, except you supply indexes rather than names\n",
    "        - `df.iloc[row, column, stride]`\n",
    "        - supply `row`, `column`, `stride` values within `[]` as a list to return a dataframe\n",
    "    - can join with boolean indexing\n",
    "        - `df.iloc[:, [True, False, True, False]]`\n",
    "            - returns the columns at indexes 0 and 2\n",
    "            - list of booleans must match the number of columns\n",
    "    - **Lambda Functions**\n",
    "        - `df.iloc[lambda x: x.index % 2 == 0]`\n",
    "            - selects only even rows\n",
    "        - `df.iloc[:, lambda df: [0, 2]]`\n",
    "            - selects all rows and columns at indexes 0 and 2\n",
    "    - Examples:\n",
    "        - `df.iloc[1, 2]`\n",
    "            - select value in second row, third column\n",
    "        - `df.iloc[[1]]`\n",
    "            - select the 2nd row as a dataframe\n",
    "        - `df.iloc[[0, 1, 2], [0, 2]]`\n",
    "            - select the first 3 rows and the first/third column as dataframe\n",
    "        - `df.iloc[:, [4, 5]]`\n",
    "            - all rows in the 5th/6th columns\n",
    "        - `df.iloc[1:3, 0:3]`\n",
    "            - rows 1 and 2, columns 0, 1, and 2\n",
    "\n",
    "#### Zeros and NaNs\n",
    "- Select only cols with all nonzero values\n",
    "    - `df.loc[:, df.all()]`\n",
    "- Select only cols with any nonzero values\n",
    "    - `df.loc[:, df.any()]`\n",
    "- Select only cols with all null values\n",
    "    - `df.loc[:, df.isnull().all()]`\n",
    "- Select only cols with any null values\n",
    "    - `df.loc[:, df.isnull().any()]`\n",
    "- Select only cols with no null values\n",
    "    - `df.loc[:, df.notnull().all()]`\n",
    "- Select only cols with any non-null values\n",
    "    - `df.loc[:, df.notnull().any()]`\n",
    "- Counting nulls\n",
    "    - `df.isnull()` will create a dataframe of 'True' 'False' values\n",
    "    - `df.isnull().sum()` will add all of the nulls up (True=1)\n",
    "        - compare results to `df.shape` to see how much is null\n",
    "- Remove nulls\n",
    "    - `df.dropna(how=)` works on rows by default\n",
    "        - `how='any'` will drop any **rows** that have at least 1 null value\n",
    "        - `how='all'` requires all values in a row to be null for dropping\n",
    "        - `thresh=size`\n",
    "            - will drop all with 'size' or more number of null values\n",
    "        - `axis='columns'` will drop columns rather than rows\n",
    "        - `subset=[]`\n",
    "            - provide a list of columns that only drops rows that are missing the specified columns\n",
    "            - i.e. `subset['a', 'b']` will drop rows if there are nulls in either col 'a' or col 'b'\n",
    "            \n",
    "#### Dropping Columns\n",
    "- You can drop entire columns\n",
    "    - store the result in a new df\n",
    "- `df_dropped = df.drop(drop_col_list, axis='columns')`\n",
    "    - easy way to subset data if only wanting certain columns\n",
    "    - add `inplace=True` to drop without having to reassign to variable\n",
    "\n",
    "#### Boolean Indexing\n",
    "- Combine conditionals with indexing\n",
    "- See above for **combining with iloc**\n",
    "- Return a series object full of booleans\n",
    "    - `df['column_name']conditional`   # conditional something like `< value`\n",
    "        - can supply whatever conditional you choose\n",
    "        - must use this 'bracket' syntax when col names have special chars like '/'\n",
    "    - `df.column_name` also works when 'column_name' doesn't have special chars in it\n",
    "- Can use the series object as the index to return a dataframe that only selects 'True' values\n",
    "    - Use directly as the index\n",
    "        - `df[df.column_name < value]`\n",
    "    - Store in a variable first\n",
    "        - `bool_index = df.column_name = value`  # returns a series object\n",
    "        - `df_new = df[bool_index]` # returns a dataframe\n",
    "    - Select only certain cols while filtering using others\n",
    "        - `df.col1[df.col2 > x]`\n",
    "- Logical operators\n",
    "    - Use numpy logical and/logical or\n",
    "        - `bool_index = np.logical_and(df.column_name > 8, df.column_name < 20)`  # returns a series object\n",
    "        - `df_new = df[bool_index]`  # returns a dataframe\n",
    "        - Use `np.logical_or` the same way\n",
    "    - Use '&', '|' instead of np logicals\n",
    "        - `df[(df.col1 == x) & (df.col2 > y)]` like `np.logical_and`\n",
    "        - `df[(df.col1 == x) | (df.col2 > y)]` like `np.logical_or`\n",
    "    - Use '!' for not\n",
    "        - `df[df.col != x]` where 'not equal' to 'x'\n",
    "    \n",
    "#### Assigning New Dataframe Using Boolean Indexing on Data\n",
    "- `filter = df.col == value` or `df['col'] == value`\n",
    "    - 'value' can be a string used to divide the dataframe (like 'SC' or 'setosa')\n",
    "- `df_filtered = df.loc[filter, :]`\n",
    "    - extracts the filtered data into a new dataframe\n",
    "- All at once\n",
    "    - `df_filtered = df[df.col == value]`\n",
    "    \n",
    "#### Assign New Values to a Dataframe Based on a Condition\n",
    "- `df.col1[df.col2 < x] = np.nan`\n",
    "    - assigns 'NaN' to 'col1' when 'col2' is less than 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Colors   Spanish  Length_Colors  Length_Spanish\n",
      "0   Brown      Cafe              5               4\n",
      "1   Green     Verde              5               5\n",
      "2   Black     Negro              5               5\n",
      "3  Yellow  Amarillo              6               8\n",
      "\n",
      "   Colors   Spanish  Length_Colors  Length_Spanish\n",
      "0   Brown      Cafe              5               4\n",
      "1   Green     Verde              5               5\n",
      "2   Black     Negro              5               5\n",
      "3  Yellow  Amarillo              6               8\n",
      "\n",
      "   Colors   Spanish  Length_Colors  Length_Spanish\n",
      "0   Brown      Cafe              5               4\n",
      "3  Yellow  Amarillo              6               8\n",
      "\n",
      "   Length_Colors  Length_Spanish\n",
      "1              5               5\n",
      "2              5               5\n",
      "3              6               8\n",
      "\n",
      "   Colors   Spanish\n",
      "2   Black     Negro\n",
      "3  Yellow  Amarillo\n",
      "Wall time: 36.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a dataframe from a dictionary\n",
    "colors = ['Brown', 'Green', 'Black', 'Yellow']\n",
    "spanish = ['Cafe', 'Verde', 'Negro', 'Amarillo']\n",
    "length_colors = [5, 5, 5, 6]\n",
    "length_spanish = [4, 5, 5, 8]\n",
    "dict1 = {'Colors': colors, 'Spanish': spanish, 'Length_Colors': length_colors, 'Length_Spanish': length_spanish}\n",
    "df = pd.DataFrame(dict1)\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# alt syntax to stitch a dataframe from lists\n",
    "# using same lists above\n",
    "# this method is more scalable\n",
    "labels = ['Colors', 'Spanish', 'Length_Colors', 'Length_Spanish']\n",
    "cols = [colors, spanish, length_colors, length_spanish] # list of lists\n",
    "zipped = list(zip(labels, cols))\n",
    "dict2 = dict(zipped)\n",
    "df2 = pd.DataFrame(dict2)\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "# select all columns and rows that have spanish != 5\n",
    "bool_index = df['Length_Spanish'] != 5\n",
    "print(df[bool_index])\n",
    "print()\n",
    "\n",
    "# select both length cols where Length_Colors is 5 or more and Length_Spanish is 5 or more\n",
    "bool_index = np.logical_and(df['Length_Colors'] >= 5, df['Length_Spanish'] >= 5)\n",
    "df_subset = df[bool_index]\n",
    "print(df_subset.loc[:, ['Length_Colors', 'Length_Spanish']])\n",
    "print()\n",
    "\n",
    "# select the last two rows and first two cols\n",
    "print(df.iloc[2:,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTypes\n",
    "- Memory usage\n",
    "    - `df.memory_usage()` can also run on a specific column\n",
    "        - `index=False` default is true\n",
    "            - displays index memory usage as well\n",
    "        - `deep=True` specifically looks into 'object' dtypes\n",
    "- There are a myriad of reasons to use the proper dtype\n",
    "    - math, datetime operations, categories use less memory and run faster, bools are special too\n",
    "- `df.dtypes`\n",
    "    - will print your datatypes for each column\n",
    "- `df.column_name.dtype` or `df['column name'].dtype`\n",
    "    - will print the dtype for a single column\n",
    "- dtypes\n",
    "    - object\n",
    "        - `dtype('O')` is object dtype, which means numbers can be stored as strings (not what you want)\n",
    "    - float\n",
    "        - can do math\n",
    "    - int\n",
    "        - can do math\n",
    "    - datetime\n",
    "        - use `df['col_name'] = pd.to_datetime(df['col_name'])`\n",
    "    - bool\n",
    "        - t/f\n",
    "    - category\n",
    "        - use whenever you can\n",
    "        - use on 'object' dtypes when the data are strings with relatively few unique values\n",
    "        - `df['col'] = pd.Categorical(df.col, ordered=True, categories=cat_list)`\n",
    "            - you should first define a list of categories in ascending order in `cat_list`\n",
    "            - `ordered=False` by default\n",
    "                - ordering allows the use of comparison operators < & > \n",
    "- Changing dtypes\n",
    "    - `df['column_name'] = df.column_name.astype('dtype')`\n",
    "        - must use bracket notation on the left to overwrite a series\n",
    "        - see 'category' section above for extra args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames\n",
    "#### Reading Multiple Files\n",
    "- Reading multiple files using a loop or comprehension\n",
    "    ```python\n",
    "    filenames = ['file1.csv', 'file2.csv']\n",
    "\n",
    "    dataframes = [pd.read_csv(f) for f in filenames]\n",
    "    ```\n",
    "- Using the glob module\n",
    "    - `from glob import glob`\n",
    "    - `filenames = glob('filename*.csv')`\n",
    "        - will grab every file that has the above format with other char(s) in the wildcard '\\*' spot\n",
    "    - use the `filenames` iterable object in a list comprehension as above\n",
    "#### Appending and Concatenating Series and Dataframes\n",
    "#### Append\n",
    "- `series1.append(series2)`\n",
    "    - will append series2 data to the end of series1\n",
    "    - columns matching\n",
    "        - if columns match and the index name matches\n",
    "            - will stack data nicely\n",
    "        - if columns don't match and/or index name is different\n",
    "            - will stack, will not merge records in both df/series\n",
    "            - will add NaN for the columns not in each respective df/series\n",
    "    - also workes with dataframes\n",
    "    - keeps the index from the original df/series\n",
    "        - can result in duplicate indexes, so may need to `.reset_index(drop=True)`\n",
    "        - `drop=True` discards the old index with repeated entries\n",
    "        \n",
    "#### **Concat**\n",
    "- `df_complete = pd.concat([df1, df2, df3...][, ignore_index=True][, axis=][,keys=][, join=])`\n",
    "    - accepts a list of dataframes or series to add together\n",
    "    - using `ignore_index=True` adds only values and increments the index int\n",
    "        - be careful with this if index is more than just an integer\n",
    "        - default is False, which keeps original index\n",
    "    - `axis='rows'` by default and will stack the df/series\n",
    "        - same as `axis=0`\n",
    "        - will stack the df/series even if the indexes match, and can repeat index labels\n",
    "            - can specify `keys=[]` arg to add an index level to distinguish rows\n",
    "    - `axis='columns'` will add columns to the df trying to match to the correct index\n",
    "        - same as `axis=1`\n",
    "        - will add NaN values for cols in the first df if they don't exist\n",
    "        - will match on index value even if indexes have different names in the respective df's\n",
    "        - not sure if this works with series or not\n",
    "    - `keys=`\n",
    "        - adds an outer index level\n",
    "        - can supply a value or a list of values to use as an index for each concatted df/series\n",
    "        - length/order of the list should be the same as the number of items you are concatenating\n",
    "        - can add a second level index, useful when using `axis=0`\n",
    "        - will also add an outer index level for columns, useful when column names match\n",
    "    - use a dictionary to specify `keys` argument\n",
    "        - `dict_keys = {key1: df1, key2: df2}`\n",
    "        - `pd.concat(dict_keys, axis=1)` will add `key1` and `key2` as outer indexes above column names\n",
    "    - `join=` \n",
    "        - default is 'outer' in which all data is preserved, and NaN's are filled for missing areas\n",
    "        - `join='inner'` will only join where data are matched, just like SQL inner join\n",
    "        \n",
    "#### **Merge**\n",
    "- Use merge when you need to match your dataframes on a column other than the index\n",
    "    - or if you need more advanced control over the merging of dataframes\n",
    "- `pd.merge(left=df1, right=df2[, on=][, how=][, suffixes=])`\n",
    "    - default is `axis=1` trying to join all columns into one df by matching rows\n",
    "    - default is `join='inner'` only matching columns in all df's\n",
    "    - `on='column_name'` **or use** `left_on='', right_on=''` **when column_names are different**\n",
    "        - specifies the column(s) to match\n",
    "            - supply a list to match on multiple columns\n",
    "            - can supply lists of equal length to `left_on` and `right_on` when needed\n",
    "        - default action is to try to match all common columns\n",
    "            - if the columns are the same in all df's, then will try to match all columns\n",
    "        - columns that are shared will have names modified by appending '\\_x' or '\\_y' etc.\n",
    "    - `suffixes=[]`\n",
    "        - supply a list of suffixes in order (length should be the same as the number of df's merging\n",
    "        - will use the suffixes supplied rather than '\\_x' or '\\_y' for shared column names\n",
    "    - `how=''`\n",
    "        - `how='inner'` by default, doing an inner join\n",
    "        - `how='left'` works just like a 'LEFT JOIN' with SQL\n",
    "            - rows are matched when they match, when they don't NaN's are filled\n",
    "        - `how='right'` works just like a 'RIGHT JOIN' with SQL\n",
    "        - `how='outer'` keeps all rows from both df's, filling NaN's when not matched\n",
    "- `pd.merge_ordered()`\n",
    "    - works similarly to `pd.merge()` but will order the results\n",
    "    - **default `how()` for this type is *'outer'*!\n",
    "    - **Filling NaNs**\n",
    "        - `fill_method=` can be specified\n",
    "            - 'ffill' and other types of fills accepted to deal with NaN's\n",
    "- `pd.merge_asof()`\n",
    "    - sort of like `merge_ordered()` but...\n",
    "        - only keeps right df rows whose 'on' values are less than the left df values\n",
    "\n",
    "#### Pandas `.join()`\n",
    "- `.join()` works similarly to `.merge()`\n",
    "    - `df1.join(df2[, how=])`\n",
    "        - default `how=` is 'left' and works just like `pd.merge(how='left')`\n",
    "        - other `how=` types work just like `.merge()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with String Data\n",
    "- Can chain `.str` methods to work with string data\n",
    "- `df['str_col'].str.upper()`\n",
    "    - returns a series (does not modify the data in place) with all caps\n",
    "    - `df.index = df.index.str.upper()` will convert index strings to all caps\n",
    "- `df['str_col'].str.contains(substring)`\n",
    "    - returns a series of True/False values based on whether each entry contains the supplied substring\n",
    "    - chain `.sum()` to the end of this, to get a count of the number of occurrences\n",
    "        - since True = 1\n",
    "- `df['str_col'] = df['str_col'].str.strip()`\n",
    "    - strip whitespace from strings\n",
    "    - works on column names, just use `df.columns`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Category DType\n",
    "- `df.col_name.value_counts()` returns each unique value and the number of occurrences in the column\n",
    "    - `normalize=True` will return the percentage as a decimal of each occurrence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Numbers\n",
    "- May need to clean some data\n",
    "    - `df['column'] = pd.to_numeric(df['column'], errors='coerce')`\n",
    "        - `errors='coerce'` will force the conversion adding NaN for non-numerics\n",
    "        \n",
    "#### Broadcasting Mathematical Operations\n",
    "- Example: a series with two cols of numeric data, and a second series with one col of numeric data\n",
    "    - `ser_w2.divide(ser_w1, axis='rows')`\n",
    "        - axis='rows', because want to divide both entries in row 1, by the number in row 1 in the second df\n",
    "- Some functions\n",
    "    - `.add()`\n",
    "    - `.divide()`\n",
    "    - `.pct_change()`\n",
    "        - calculates the pct change as a decimal from the previous entry\n",
    "    - options\n",
    "        - `fill_value=` default is NaN, which results in NaNs if only 1 value is missing\n",
    "\n",
    "#### Simple arithematic\n",
    "- can just add, subtract etc. df or series\n",
    "    - will match on indexes, but return NaN along with indexes that aren't found in every df/series\n",
    "- recall `//` will return the result of division, truncating the value if there is a remainder\n",
    "    - i.e. `1983 // 10 = 198`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Datetimes\n",
    "- `pd.to_datetime(list, format='')`\n",
    "    - convert a list of values to datetimes\n",
    "    - can supply a format string (using standard format chars like %Y-%m ...)\n",
    "    - may need to convert the data to string format first and format properly\n",
    "        - `df['date'] = df['date'].astype(str)`\n",
    "        - `df['time'] = df['time'].apply(lambda x: '{:0>4}'.format(x))`\n",
    "            - pad leading zeros on time if necessary\n",
    "        - `dt_string = df['date'] + df['time']`\n",
    "        - `date_times = pd.to_datetime(dt_string, '%Y/%m/%d%H%M')`\n",
    "        - `df_clean = df.set_index(date_times)`\n",
    "- `df['date_col'].dt.hour`\n",
    "    - can use datetime attributes to access pieces of a datetime from each value\n",
    "        - this example will return a series containing only the hour for each datetime\n",
    "- Combine dates and times into one column when in separate columns\n",
    "    - concat them as strings then assign to df column\n",
    "        - `combined = df.date_col.str.cat(df.time_col, sep=' ')`\n",
    "        - `df['dt'] = pd.to_datetime(combined)`\n",
    "        - `df.set_index('dt', inplace=True)` set the datetime to the index if desired\n",
    "- Convert Timezones\n",
    "    - make datetimes 'aware' by setting a local timezone\n",
    "        - `aware_dates = df['date_col'].dt.tz_localize(timezone_string)`\n",
    "            - where `timezone_string` is in the proper format ('US/Central')\n",
    "    - convert datetiems\n",
    "        - `eastern_dates = aware_dates.dt.tz_convert('US/Eastern')`\n",
    "    - can chain the entire thing together (must repeat the `.dt.` part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations (Vectorized Computations)\n",
    "- The methods below return a dataframe without modifying the original\n",
    "    - the results must be stored, saved, or used\n",
    "    - can store in the dataframe being manipulated by assigning to a new column\n",
    "- Pandas Built-ins\n",
    "    - Simply do arithmetic on cols\n",
    "        - `df['sum_of_1_2'] = df.col1 + df.col2`\n",
    "    - Apply a custom function to a dataframe (does not work on the index, see map below)\n",
    "        - `df.apply(function)`\n",
    "            - you must first define your function or supply a lambda function\n",
    "            - supply function name without '()' or args\n",
    "            - function must accept one arg, and that arg is each entry in the df\n",
    "    - Apply a custom function to the index of a dataframe\n",
    "        - `df.index.map(function)`\n",
    "            - does not modify the index in place\n",
    "                - must `df.index = df.index.map(function)`\n",
    "            - function works like with `.apply`\n",
    "                - no '()' or args passed\n",
    "                - must accept one arg (each entry of the index)\n",
    "                - lambda functions ok\n",
    "    - 'Map' dictionary values to keys found in the df (**map method**)\n",
    "        - `df['new_col'] = df.col.map(dict)`\n",
    "            - will search `df.col` for the keys in the dictionary\n",
    "            - will add the dict 'value' to `df.new_col` for each row corresponding to its 'key' in `df.col`\n",
    "    - Divide by a number and round down\n",
    "        - `df.col.floordiv(num)`\n",
    "            - divides all values by the supplied 'num' and rounds down\n",
    "            - useful for questions like 'how many dozen' (12 for num)\n",
    "- Numpy ufuncs\n",
    "    - Divide by a number and round down\n",
    "        - `np.floor_divide(df, num)`\n",
    "            - works like pandas `.floordiv()` above\n",
    "\n",
    "#### Transforming Using groupby\n",
    "- Can apply a transform function to a grouped set\n",
    "    - `df.groupby('col_name')['value'].transform(func)`\n",
    "        - will alter the data in place using the supplied 'function'\n",
    "            - this function should accept a series and return a series\n",
    "                - aka, it takes a value and transforms it, then does so for the next value\n",
    "                - example `zscore`, which is the # of std away from the mean for each value\n",
    "                - will transform each value in the 'value' column, grouped by 'col'\n",
    "                \n",
    "```python\n",
    "def zscore(series):\n",
    "    return (series - series.mean()) / series.std()\n",
    "df.groupby('col')['value'].transform(zscore)\n",
    "```\n",
    "- can use `.apply(func)` when using more complex functions rather than `.transform()`\n",
    "    - example to transform one column and return the whole df\n",
    "```python\n",
    "            def zscore_details(group):\n",
    "                df = pd.DataFrame(\n",
    "                        {'zcol1': zscore(group['col1']),\n",
    "                         'col2': group['col2'],\n",
    "                         'col3': group['col3']})\n",
    "            return df\n",
    "        \n",
    "            df.groupby('col4').apply(zscore_details)\n",
    "```\n",
    "- `col2` and `col3` are returned as is, but col1 is transformed  \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "- Often used with datetimes for summary info\n",
    "    - mean, count, sum, etc.\n",
    "- `df.resample(freq)` usually chained with a stat method call\n",
    "    - `freq` is a string to specify the frequency you want\n",
    "        - 'min' or 'T' for minute\n",
    "        - 'H' for hourly\n",
    "        - 'D' for daily\n",
    "        - 'B' for business day\n",
    "        - 'W' for weekly\n",
    "        - 'M' for monthly\n",
    "        - 'Q' for quarterly\n",
    "        - 'A' for annually\n",
    "    - can add an integer to specify every 2 or 3 days etc.\n",
    "        - `df.loc[:,'col'].resample('2W').mean()`\n",
    "    - should chain summary functions to the resample call\n",
    "        - `df['col'].resample('D').mean()`\n",
    "- Chaining multiple methods is possible\n",
    "    - `df.resample('M').sum().max()`\n",
    "        - returns the maximum monthly sum\n",
    "- Downsampling\n",
    "    - reducing datetime rows to a slower frequency (yearly to monthly)\n",
    "    - no additional methods need to be chained beyond those desired\n",
    "- Upsampling\n",
    "    - increasing the frequency (weekly to daily)\n",
    "    - need to tell pandas how to fill the extra data\n",
    "    - `df.loc['yyyy-dd-mm':'yyyy-dd-mm', 'col'].resample('4H').ffill()`\n",
    "        - will make the time appear every 4 hours, (even if all you had was daily)\n",
    "        - will forward fill using values from previous times until a new value is encountered\n",
    "            - good for running totals\n",
    "        - `fill_method` options\n",
    "            - `ffill` forward fill\n",
    "            - `bfill` backward fill\n",
    "            - `pad` in between forward and backward\n",
    "            - `first` only keeps the actual values, and fills with NaN values\n",
    "    - interpolating data rather than filling\n",
    "        - `df.resample('A').first().interpolate(how='linear')`\n",
    "            - will use a linear interpolation to make a coarse time series yearly\n",
    "        - use `first` as the fill method for upsampling\n",
    "        - use `.interpolate(how=type)` to specify how to fill\n",
    "            - where `type` is a string specifying how to fill\n",
    "- Rolling Mean\n",
    "    - `data.rolling(window=).mean()`\n",
    "        - calculates a smooth 'rolling' mean for you data or data slice\n",
    "        - `window=24` will compute new values for each hourly point\n",
    "            - based on a 24 hour window stretching out behind each point\n",
    "        - `window=7` after a daily resample will do it daily\n",
    "            - still trying to figure out how the `window` arg works\n",
    "            - `data.resample('D').max().rolling(window=7).mean()`\n",
    "                - calculates the rolling mean over 7 days of the daily maximum (I think)\n",
    "                - so you still get a daily max, but not until day 7\n",
    "                    - means are smoothed by using 7 daily maxes prior to each day in the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "- Works like numpy broadcasting\n",
    "- Syntax:\n",
    "    - `df['column_name'] = value`\n",
    "        - every row in 'column_name' has value of 'value' now\n",
    "        - this 'column_name' can be a new or existing column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Through Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Column Labels\n",
    "    - `for i in df: statements`\n",
    "- Rows  # need to use `.iterrows()` method\n",
    "    - `for index, row in df.iterrows(): statements`\n",
    "        - `index` refers to the row labels\n",
    "        - `row` refers to a series object including col name and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors\n",
      "Spanish\n",
      "Length_Colors\n",
      "Length_Spanish\n"
     ]
    }
   ],
   "source": [
    "# using the df created above\n",
    "# note that there is no col label for the row labels\n",
    "\n",
    "for i in df:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: 0     Spanish: Cafe        Spanish Length: 4\n",
      "Row: 1     Spanish: Verde       Spanish Length: 5\n",
      "Row: 2     Spanish: Negro       Spanish Length: 5\n",
      "Row: 3     Spanish: Amarillo    Spanish Length: 8\n"
     ]
    }
   ],
   "source": [
    "# access columns\n",
    "# .ljust(width) helps with alignment and spacing\n",
    "for index, row in df.iterrows():\n",
    "    print(('Row: ' + str(index)).ljust(10), ('Spanish: ' + str(row[1])).ljust(20), 'Spanish Length: ' + str(row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "15\n",
      "15\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# loop through a column's values\n",
    "for index, row in df.iterrows():\n",
    "    print(row[3] + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a New Column Based on a Calculation\n",
    "- Using `.iterrows()` can do the same thing, but it's less efficient and best on small dataframes\n",
    "- `df['new_column'] = df['column'].apply(function)`\n",
    "    - supply a new column name and a function to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     brown\n",
      "1     green\n",
      "2     black\n",
      "3    yellow\n",
      "Name: color_lower, dtype: object\n",
      "\n",
      "0    15\n",
      "1    15\n",
      "2    15\n",
      "3    16\n",
      "Name: length_plus_ten, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a new column from the calculation above\n",
    "df['color_lower'] = df['Colors'].apply(str.lower)\n",
    "print(df['color_lower'])\n",
    "print()\n",
    "\n",
    "# apply a numeric function to a column to create a new column\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "df['length_plus_ten'] = df['Length_Colors'].apply(add_ten)\n",
    "print(df['length_plus_ten'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data Frames\n",
    "- Melt, pivot, and pivot table are outlined in the Data Cleaning section of the Data Science file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With File Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a DataFrame into a File\n",
    "- `df.to_csv(filename[, sep])`\n",
    "    - where `filename` is a string of your filename\n",
    "    - `sep` can set a delimiter to other than comma (default)\n",
    "        - `sep='\\t'` for tab separated (use '.tsv' rather than '.csv' in file name)\n",
    "- `df.to_excel(filename)`\n",
    "    - name `filename` with '.xlsx' extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Flat Files\n",
    "- Such as csv and txt files with rows and cols\n",
    "- `dataframe = pd.read_csv(filename[, sep][, comment][, na_values][, nrows][, header][, names][, parse_dates)`\n",
    "    - `index_col='col'`\n",
    "        - can also supply a column name to use as the index\n",
    "        - useful when parsing dates (see below)\n",
    "    - `sep` is pandas version of delimiter with default `','`\n",
    "    - `comment` takes the char that comments appear after (for python it's '#')\n",
    "    - `na_values` \n",
    "        - takes a list of strings to identify/replace with NaN\n",
    "            - blank spaces preceding values in the data can affect this\n",
    "            - so check for them if experiencing issues\n",
    "        - can accept a dictionary using column names as indexes and lists of values to replace for the value\n",
    "            - `na_values={'col1':['  -1', ' -1', '-1'], 'col2':['no_data', 'N/A']}`\n",
    "    - `nrows` specifies an integer for the number of rows to retrieve\n",
    "    - `header=None` if no header\n",
    "        - `names=col_names` where 'col_names' is a list of header names for each column\n",
    "        - the supplied list should have the same length as the number of columns\n",
    "    - `parse_dates=[[index1, index2, index3]]`\n",
    "        - can also set `parse_dates=True` and see what happens\n",
    "            - combine with `index_col='date'` to index by these dates\n",
    "        - intelligently parses the date entries from each supplied index and combines them into one datetime\n",
    "        - use integers for the indexes (i.e. `parse_dates=[[4, 5]]`) to specify output of parsing\n",
    "        - use a column name for the indexes (i.e. `parse_dates=[['year', 'month']]`)\n",
    "            - use a single column name to keep the datetime together `parse_dates=['date']`\n",
    "        - it may even parse the column name, need to test\n",
    "    - view the header and first 5 lines of the dataframe with `.head()` method `dataframe.head()`\n",
    "    \n",
    "    - **Parse the date and set as index during import**\n",
    "        - set `index_col='date'` and `parse_dates=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating Through Large Files\n",
    "- Simple example using chunking to record each unique value and it's number of occurrences\n",
    "    - Initialize empty dictionary\n",
    "        - `dict1 = {}`\n",
    "    - Iterate over the file\n",
    "          `for chunk in pd.read_csv(filevariable, chunksize=100):`\n",
    "              `# iterate over a column in the file`\n",
    "              `for entry in chunk['col_name']:`\n",
    "                  `if entry in dict1.keys():`\n",
    "                      `dict1[entry] += 1`\n",
    "                  `else:`\n",
    "                      `dict1[entry] = 1`\n",
    "    - Convert to a dataframe\n",
    "        - `df = pd.DataFrame(dict1)`\n",
    "\n",
    "\n",
    "#### Complex Example\n",
    "- Use a reader object to read the files a specific number of lines at a time\n",
    "    - `file_name_reader = pd.read_csv('filename', chunksize=num)`\n",
    "        - common to store filename in a variable and use the variable\n",
    "        - *num* is the number of lines to read, 1000 is a good number\n",
    "- Initialize empty df\n",
    "    - `data = pd.DataFrame()`\n",
    "- Iterate over each chunk\n",
    "    - `for grp in file_name_reader:` \n",
    "          `filtered_data = grp[grp['col_of_interest'] == condition]`\n",
    "          # exclude all data not meeting condition\n",
    "- Zip any columns you want\n",
    "    - `data_zip = zip(filtered_data['col_name1'], filtered_data['col_name2])`\n",
    "- Convert zip object to a list\n",
    "    - `data_list = list(data_zip)`\n",
    "- Create new dataframe column (this example does a calculation on the two columns to get a %)\n",
    "    - use list comprehension if needed to create your new column\n",
    "        - `filtered_data['new_column'] = [int(tup[0] * tup[1] * 0.01) for tup in data_list]`\n",
    "- Append this 'chunk' to the dataframe\n",
    "    - `data = data.append(filtered_data)\n",
    "- Can nest this entire thing in a function to call by supplying relatively few parameters\n",
    "    - DataCamp example similar to this in old `Python Library.docx` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel Files\n",
    "- `datafile = pd.ExcelFile('filename')`\n",
    "- view different sheets in the file/dataframe\n",
    "    - `print(datafile.sheet_names)`\n",
    "    - use `.sheet_names` attribute of this object\n",
    "- extract a sheet into a dataframe\n",
    "    - `dataframe = datafile.parse(sheet[, skiprows][, names][, usecols])`\n",
    "        - `sheet` supply sheet name as a str or index as float (0 indexed)\n",
    "        - the following args must be in list format\n",
    "            - `[arg]` if only supplying one value\n",
    "        - `skiprows` supply a list of rows to skip (0 indexed)\n",
    "        - `names` supply a list of names for your imported columns\n",
    "        - `usecols` supply a list of columns to import (0 indexed)\n",
    "- Read Excel file and store each sheet as a dataframe with sheet names as the keys to each individual dataframe\n",
    "    - `df = pd.read_excel('filename', sheetname=none)`\n",
    "        - can specify a 'sheet' or if sheet='none' will save all sheets using sheet names as keys\n",
    "        - can use a 'url' as the 'filename' to scrape data from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAS and Stata Files\n",
    "- SAS Files\n",
    "    - `from sas7bdat import SAS7BDAT`\n",
    "    - `with SAS7BDAT('filename.sas7bdat') as file:`\n",
    "          `dfsas = file.to_data_frame()`\n",
    "- Stata Files\n",
    "    `data = pd.read_stata('filename.dta')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDF5 Files\n",
    "- HDF5 is becoming the industry standard for big data sets\n",
    "- hierachy of key values, where a value here then becomes a key\n",
    "- `import h5py`\n",
    "  `filename = filename.hdf5`\n",
    "  `data = h5py.File(filename, 'r')`\n",
    "- exploring data structure\n",
    "    - `for key in data.keys():`\n",
    "      `print(key)`\n",
    "    - provides keys that can be accessed such as 'meta' for metadata\n",
    "    - access its contents\n",
    "        - `for key in data['meta'].keys():`\n",
    "          `print(key)` returns another key in this example 'Description'\n",
    "    - accessing values\n",
    "        - `data['meta']['Description'].value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Data fro the Web\n",
    "- Some functionality using the 'urllib' package\n",
    "    - `from urllib.request import urlretrieve, urlopen, Request`\n",
    "    - import not necessary when using some of the functions below\n",
    "- Import data into a dataframe using a url\n",
    "    - `url = 'http://....filename.csv'`\n",
    "    - `df = pd.read_csv(url, sep=';')` using the appropriate separator (delimiter)\n",
    "    - `df = pd.read_excel(url, sheetname=none)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Databases\n",
    "- Need to import the appropriate package\n",
    "    - `from sqlalchemy import create_engine`\n",
    "- Creating an engine (sqlalchemy package)\n",
    "    - `engine = create_engine('sqlite:///db_name.sqlite')`\n",
    "        - above syntax `'db_type:///db_name.extension'`\n",
    "- Running a query using Pandas\n",
    "    - `df = pd.read_sql_query(\"SELECT * FROM table_name\", engine)\n",
    "    - `engine` is the engine to connect to (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Concatenating Data into a Dataframe from Many Files\n",
    "#### Concatenating Dataframes Using Pandas\n",
    "- Useful when combining data sources\n",
    "- `df_concat = pd.concat([df1, df2], axis=0,ignore_index=True)`\n",
    "    - works well when your dataframes have the same columns in the same order\n",
    "        - adds rows, keeping your columns\n",
    "    - `axis` is optional\n",
    "        - `axis=0` is default, and adds rows to your columns\n",
    "        - `axis=1` will add new columns on the right of the dataframe, matching on row index\n",
    "    - `ignore_index` is optional\n",
    "        - default is `ignore_index=False` and keeps original index values (produces duplicate row indexes)\n",
    "        - `ignore_index=True` will reindex the new dataframe\n",
    "- Use `glob` to find files based on a pattern\n",
    "    - need to `import glob`\n",
    "    - useful when trying to process thousands of files for concatenation\n",
    "    - uses **wildcards** to help matching\n",
    "        - `*` matches zero or more of any char\n",
    "        - `?` matches any single char in that position\n",
    "        - `[ ]` matches chars specified within\n",
    "            - `[0-9]` matches number 0-9\n",
    "            - `[09]` mathces 0 and matches 9\n",
    "    - creates a list of file names that match your pattern\n",
    "    - Example:\n",
    "        - `csv_files = glob.glob('*.csv')` will store a list of all csv files\n",
    "- Example: to combine these skills to create a large dataframe from many files\n",
    "    - `list_data = []`\n",
    "    - `for filename in csv_files:`\n",
    "        - `data = pd.read_csv(filename)`\n",
    "        - `list_data.append(data)`\n",
    "            - this results in a list of dataframes, which can be loaded into `pd.concat`\n",
    "    - `df = pd.concat(list_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
