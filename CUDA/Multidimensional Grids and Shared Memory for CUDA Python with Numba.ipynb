{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Grids and Shared Memory for CUDA Python with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will introduce a few more intermediate techniques for programming in CUDA with Numba. First we will introduce how CUDA provides the ability to create 2 and 3 dimensional blocks and grids which can ease programming when working with 2 and 3 dimensional data. Next, we will introduce an on-chip, programmer managed memory space called **shared memory** which can be used for very fast communication between threads within a given block. You will have a chance to use both of these techniques to optimize a 2D matrix multiplication kernel.\n",
    "\n",
    "This section also provides an appendix with an example of reducing **shared memory bank conflicts** for a matrix transpose algorithm, with a link to resources for further study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the time you complete the required parts of this section you will be able to:\n",
    "\n",
    "* Do GPU accelerated parallel work on multidimensional data sets using multi dimensional blocks and grids.\n",
    "* Use shared memory to cache data on chip and reduce slow global memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 and 3 Dimensional Blocks and Grids\n",
    "\n",
    "Both grids and blocks can be configured to contain a 2 or 3 dimensional collection of blocks or threads, respectively. This is done mostly as a matter of convenience for programmers who often work with 2 or 3 dimensional datasets. Here is a very trivial example to highlight the syntax. You may need to read *both* the kernel definition and its launch before the concept makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.zeros(16).reshape(4, 4).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "@cuda.jit\n",
    "def add_2D_coordinates(A):\n",
    "    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    A[y][x] = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n",
    "# by using a Python tuple to signify grid and block dimensions.\n",
    "blocks = (2, 2)\n",
    "threads_per_block = (2, 2)\n",
    "\n",
    "add_2D_coordinates[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside About Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important always to remember that GPUs perform well when they are given large enough grids to support SMs in always having additional work to do while they wait on operations with latency to expire. With that in mind it's worth mentioning that several of the exercises in this section will ask you to write kernels that do not follow this best practice. This is done to give you a chance to focus on the syntax and some of the basic patterns of working in multiple dimensions, which tends to take a bit of orientation, even without doing meaningful work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add 2D Matrices on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will modify a kernel definition and its launch configuration to perform 2D matrix addition in parallel. To start with you'll be writing a naive kernel that will only work when launched with a grid whose shape matches that of the data. If you get stuck feel free to refer to [the solution](../edit/solutions/add_matrix_solution.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is written to add two 1D vectors. Refactor it to add 2D matrices\n",
    "@cuda.jit\n",
    "def add_matrix(A, B, C):\n",
    "#     i = cuda.grid(1)\n",
    "    \n",
    "#     C[i] = A[i] + B[i]\n",
    "\n",
    "    i,j = cuda.grid(2)\n",
    "    \n",
    "    C[j,i] = A[j,i] + B[j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell, which defines 2D matrices of size 36*36\n",
    "A = np.arange(36*36).reshape(36, 36).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refactor the launch configuration to use a 2D grid with 2D blocks\n",
    "# blocks = 36\n",
    "# threads_per_block = 36\n",
    "blocks = (6, 6)\n",
    "threads_per_block = (6, 6)\n",
    "\n",
    "# This launch will throw a Typing error until refactor the definition above to operate on 2D arrays\n",
    "add_matrix[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multiply 2D Matrices on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll complete the logic of a kernel that calculates one element for a 2D [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) operation. Like the matrix add kernel you just wrote, this kernel is also naive in that it requires grid dimensions to match that of the passed in matrices. Refer to the [solution](../edit/solutions/matrix_multiply_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def mm(a, b, c):\n",
    "    row, column = cuda.grid(2)\n",
    "    sum = 0\n",
    "    \n",
    "    ###\n",
    "    # TODO: Build the rest of this kernel to calculate the value for one element in the output matrix.\n",
    "    ###\n",
    "    for i in range(a.shape[0]):\n",
    "        sum += a[row][i] * b[i][column]\n",
    "    \n",
    "    \n",
    "        \n",
    "    c[row][column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell\n",
    "a = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "b = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "grid = (2,2)\n",
    "block = (2,2)\n",
    "mm[grid, block](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you successfully implement the kernel\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Striding in Multiple Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we can use Numba's `cuda.gridsize(1)` to obtain the total number of threads in a grid, so too we can use `cuda.gridsize(2)` to get variables for the total number of threads in each each direction for a 2D grid. This can be useful, for example, when we have a 2D dataset larger than our grid where we would wish for each thread to stride over the grid in a loop in order to cover all necessary work.\n",
    "\n",
    "Just like with our 1D grid stride loop, this technique also allows flexibility in the sizes of our grids and blocks, regardless of the shape of our data. The following example demonstrates the use of a 2D grid stride loop, ultimately printing information about which threads in the grid are working on which elements in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def add_2D_coordinates_stride(A):\n",
    "\n",
    "    grid_y, grid_x = cuda.grid(2)\n",
    "    # By passing `2`, we get the grid size in both the x an y dimensions\n",
    "    stride_y, stride_x = cuda.gridsize(2)\n",
    "    \n",
    "    for data_i in range(grid_x, A.shape[0], stride_x):\n",
    "        for data_j in range(grid_y, A.shape[1], stride_y):\n",
    "            A[data_i][data_j] = grid_x + grid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a 2D grid with 6 blocks in a 3x2 structure, each with 6 threads in a 3x2 structure. The grid is both smaller than our total data set, and has a shape that does not fit evenly into the dataset's dimensions. Still, the kernel is able to access each element in the data. After running the cell, play around with both the data's shape, as the grid's. Try to predict what the output matrix's values will be before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros(55).reshape(11, 5).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "blocks = (3, 2)\n",
    "threads_per_block = (3, 2)\n",
    "\n",
    "# With this configuration, `stride_x` will be 9, and `stride_y` will be 4\n",
    "add_2D_coordinates_stride[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add 2D Matrices Larger than the Grid Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will modify the naive matrix addition kernel above to be able to work on datasets of an arbitrary size. If you get stuck feel free to refer to [the solution](../edit/solutions/add_matrix_stride_solution.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently this kernel will only work correctly when passed matrices that are of the same size as the grid.\n",
    "# Refactor using a strid in 2D so that it can work on data sets of an arbitrary size.\n",
    "@cuda.jit\n",
    "def add_matrix_stride(A, B, C):\n",
    "    j,i = cuda.grid(2)\n",
    "    \n",
    "    stride_j, stride_i = cuda.gridsize(2)\n",
    "    \n",
    "    for x in range(i, A.shape[0], stride_i):\n",
    "        for y in range(j, A.shape[1], stride_j):\n",
    "            C[x][y] = A[x][y] + B[x][y]\n",
    "    \n",
    "#     C[i,j] = A[i,j] + B[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't modify the values in this cell. They create a scenario where the data is\n",
    "# larger than the grid size.\n",
    "A = np.arange(64*64).reshape(64, 64).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "blocks = (6,6)\n",
    "threads_per_block = (6,6)\n",
    "\n",
    "add_matrix_stride[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Multiply 2D Matrices Larger than the Grid Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will complete a matrix multiplication kernel that will be able to work with arbitrary grid and data set shapes. You only need to work on the two lines containing the `TODO`s, using the `grid_` and `stride_` values to correctly map the work of the executing kernel into the data. Refer to the [solution](../edit/solutions/matrix_multiply_stride_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def mm_stride(A, B, C):\n",
    "\n",
    "    grid_row, grid_column = cuda.grid(2)\n",
    "    stride_row, stride_column = cuda.gridsize(2)\n",
    "    \n",
    "    for data_row in range(stride_row): # TODO: replace 0 with values that will correctly set data_row\n",
    "        for data_column in range(stride_column): # TODO: replace 0 with values that will correctly set data_column\n",
    "            sum = 0\n",
    "            for i in range(A.shape[1]): # B.shape[0] would also be okay here\n",
    "                sum += A[data_row][i] * B[i][data_column]\n",
    "                \n",
    "            C[data_row][data_column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please do not modify this cell. The strange dimensions of this data, and\n",
    "# the grid below are being set to make sure your kernel correctly handles arbitrary\n",
    "# data and grid sizes.\n",
    "\n",
    "a = np.arange(12).reshape(3,4).astype(np.int32)\n",
    "b = np.arange(24).reshape(4,6).astype(np.int32)\n",
    "c = np.zeros((a.shape[0], b.shape[1])).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "ts = (4, 3)\n",
    "bs = (3, 7)\n",
    "\n",
    "mm_stride[bs, ts](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "So far we have been differentiating between host and device memory, as if device memory were a single kind of memory. But in fact, CUDA has an even more fine-grained [memory hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy). The device memory we have been utilizing thus far is called **global memory** which is available to any thread or block on the device, can persist for the lifetime of the application, and is a relatively large memory space.\n",
    "\n",
    "As a final topic we will discuss how to utilize a region of on-chip device memory called **shared memory**. Shared memory is a programmer defined cache of limited size that [depends on the GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) being used and is **shared** between all threads in a block. It is a scarce resource, cannot be accessed by threads outside of the block where it was allocated, and does not persist after a kernel finishes executing. Shared memory however has a much higher bandwidth than global memory and can be used to great effect in many kernels, especially to optimize performance.\n",
    "\n",
    "Here are a few common use cases for shared memory:\n",
    "\n",
    " * Caching memory read from global memory that will need to be read multiple times within a block.\n",
    " * Buffering output from threads so it can be coalesced before writing it back to global memory.\n",
    " * Staging data for scatter/gather operations within a block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory Syntax\n",
    "\n",
    "Numba provides [functions](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization) for allocating shared memory as well as for synchronizing between threads in a block, which is often necessary after parallel threads read from or write to shared memory.\n",
    "\n",
    "When declaring shared memory, you provide the shape of the shared array, as well as its type, using a [Numba type](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types). **The shape of the array must be a constant value**, and therefore, you cannot use arguments passed into the function, or, provided variables like `numba.cuda.blockDim.x`, or the calculated values of `cuda.griddim`. Here is a convoluted example to demonstrate the syntax with comments pointing out the movement from host memory to global device memory, to shared memory, back to global device memory, and finally back to host memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import types, cuda\n",
    "\n",
    "@cuda.jit\n",
    "def swap_with_shared(x, y):\n",
    "    # Allocate a 4 element vector containing int32 values in shared memory.\n",
    "    temp = cuda.shared.array(4, dtype=types.int32)\n",
    "    \n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Move an element from global memory into shared memory\n",
    "    temp[idx] = x[idx]\n",
    "    \n",
    "    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n",
    "    cuda.syncthreads()\n",
    "    #...the following operation is reading an element written to shared memory by another thread.\n",
    "    \n",
    "    # Move an element from shared memory back into global memory\n",
    "    y[idx] = temp[cuda.blockDim.x - cuda.threadIdx.x - 1] # swap elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(4).astype(np.int32)\n",
    "y = np.zeros_like(x)\n",
    "\n",
    "# Move host memory to device (global) memory\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "swap_with_shared[1, 4](d_x, d_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move device (global) memory back to the host\n",
    "d_y.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Shared Memory Example: Matrix Transposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of the power of shared memory, and as a further example in 2D CUDA programming, let's write a matrix transpose kernel that takes a 2D array in row-major order and puts it in column-major order. (This is based on Mark Harris' [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post, but uses a slightly simpler algorithm).\n",
    "\n",
    "In this example, we will be using shared memory as a mechanism to allow both coalesced reads and coalesced writes to and from global memory. This would typically not be possible for a transposition algorithm, but because shared memory access is so fast, we can make \"non-coalesced\" reads from or writes to it with no performance penalty in order to then read and/or write to or from global memory in a coalesced manner. The example will demonstrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transposition Without Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before showing the shared memory implementation, let's first do a naive approach where we let each thread read and write individual elements independently using only global memory. In this way, when you see the shared memory example, you can focus on how shared memory impacts the algorithm, as opposed to the basics of the algorithm itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def transpose(a_in, a_out):\n",
    "    # Explicitly calculate indices rather than using cuda.grid(2)\n",
    "    row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    a_out[row, col] = a_in[col, row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0         1         2 ...     16381     16382     16383]\n",
      " [    16384     16385     16386 ...     32765     32766     32767]\n",
      " [    32768     32769     32770 ...     49149     49150     49151]\n",
      " ...\n",
      " [268386304 268386305 268386306 ... 268402685 268402686 268402687]\n",
      " [268402688 268402689 268402690 ... 268419069 268419070 268419071]\n",
      " [268419072 268419073 268419074 ... 268435453 268435454 268435455]]\n"
     ]
    }
   ],
   "source": [
    "size = 16384\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threads_per_block = (32, 32)\n",
    "blocks_per_grid = (int(size/threads_per_block[0]), int(size/threads_per_block[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.9 ms ± 774 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit transpose[blocks_per_grid, threads_per_block](a_in, a_out); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0     16384     32768 ... 268386304 268402688 268419072]\n",
      " [        1     16385     32769 ... 268386305 268402689 268419073]\n",
      " [        2     16386     32770 ... 268386306 268402690 268419074]\n",
      " ...\n",
      " [    16381     32765     49149 ... 268402685 268419069 268435453]\n",
      " [    16382     32766     49150 ... 268402686 268419070 268435454]\n",
      " [    16383     32767     49151 ... 268402687 268419071 268435455]]\n"
     ]
    }
   ],
   "source": [
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Transposition With Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the algorithm using shared memory. To do so:\n",
    "\n",
    "1. Each block will create a 32x32 element shared memory array\n",
    "2. Each block will make a coalesced read from the input array in global memory into the shared memory array\n",
    "3. Before writing the values back to global memory, each thread in the block will wait for all other threads in the block to complete their reads using a thread sync\n",
    "4. We make a coalesced write to global memory from shared memory, writing in a transposed order. We will do this in two steps, first transposing the location of the shared memory tile within the total input array (4a below), and then, transposing the locations of the elements within the tile before writing them back to global memory (4b below).\n",
    "\n",
    "This method allows us to make the coalesced reads and writes to and from global memory since we can perform the transposition within the shared memory space where there is no performance penalty for non-contiguous reads and writes within shared memory arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba.types\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose(a_in, a_out):\n",
    "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
    "    # and that `a_in` is a multiple of these dimensions.\n",
    "    \n",
    "    # 1) Create 32x32 shared memory array.\n",
    "    tile = cuda.shared.array((32, 32), numba.types.int32)\n",
    "\n",
    "    # Compute offsets into global input array.\n",
    "    row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    # 2) Make coalesced read from global memory into shared memory array.\n",
    "    # Note the use of local thread indices for the shared memory write,\n",
    "    # and global offsets for global memory read.\n",
    "    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a_in[col, row]\n",
    "\n",
    "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # 4a) Calculate transposed location for the shared memory array tile\n",
    "    # to be written back to global memory...\n",
    "    row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
    "\n",
    "    # 4b) ...Write back to global memory,\n",
    "    # transposing each element within the shared memory array.\n",
    "    a_out[col, row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 ms ± 62.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[        0     16384     32768 ... 268386304 268402688 268419072]\n",
      " [        1     16385     32769 ... 268386305 268402689 268419073]\n",
      " [        2     16386     32770 ... 268386306 268402690 268419074]\n",
      " ...\n",
      " [    16381     32765     49149 ... 268402685 268419069 268435453]\n",
      " [    16382     32766     49150 ... 268402686 268419070 268435454]\n",
      " [    16383     32767     49151 ... 268402687 268419071 268435455]]\n"
     ]
    }
   ],
   "source": [
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "%timeit tile_transpose[blocks_per_grid, threads_per_block](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a decent speed up on top of already accelerated code. (The [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post demonstrates a method to achieve an even higher speedup by having a thread in the block be responsible for multiple entries in a tile, which reduces the cost of indexing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "The following exercise will require you to utilize everything you've learned so far. Unlike previous exercises, there will not be any solution code available to you, and, there are a couple additional steps you will need to take to \"run the assessment\" and get a score for your attempt(s). **Please read the directions carefully before beginning your work to ensure the best chance at successfully completing the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Run the Assessment\n",
    "\n",
    "Take the following steps to complete this assessment:\n",
    "\n",
    "1. Using the instructions that follow, work on the cells below as you usually would for an exercise.\n",
    "2. When you are satisfied with your work, follow the instructions below to copy and paste code in into linked source code files. Be sure to save the files after you paste your work.\n",
    "3. Return to the browser tab you used to launch this notebook, and click on the **\"Assess\"** button. After a few seconds a score will be generated along with a helpful message.\n",
    "\n",
    "You are welcome to click on the **Assess** button as many times as you like, so feel free if you don't pass the first time to make additional modifications to your code and repeat steps 1 through 3. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Run the assessment](images/run_assess_task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiply with Shared Memory\n",
    "\n",
    "In this exercise you will complete a matrix mulitply kernel that will use shared memory to cache values from the input matrices so that they only need be accessed from global memory once, after which calculations for a thread's output element can utilize the cached values.This purpose of this assessment is to test your ability to reason about a 2D parallel problem and utilize shared memory. This particular problem doesn't have a ton of arithmetic intensity,  and we are not going to use a huge dataset so we will likely not see big speedups vs. the very simple CPU version. However, the ability to use the techniques asked of you will provide you ability in a wide number of situations where you will genuinely wish to accelerate some program involving a 2D dataset.\n",
    "\n",
    "To keep the focus on shared memory, this problem assumes input vectors of MxN and NxM dimensions with NxN threads per block and M/N blocks per grid. This means that shared memory caches with elements equal to the number of threads per block will be sufficient to provide all elements from the input matrices necessary for the calculations, and that no grid striding will be required.\n",
    "\n",
    "The following images shows the input matrices, the output matrix, a region of the output matrix that a block will calculate values for, the regions in the input matrices that this block will cache, and also, the output element and input elements for a single thread in that block:\n",
    "\n",
    "![matrix multiply diagram](images/mm_image.png)\n",
    "\n",
    "The shared memory caches have already been allocated in the kernel, your task is twofold:\n",
    "1. Use each thread in the block to populate one element in each of the caches.\n",
    "2. Use the shared memory caches in calculating each thread's `sum` value.\n",
    "\n",
    "Be sure to do any thread synchronizing that might be required to avoid cached values written by other threads not yet being available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips to Assist Your Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think Through the Spatial Challenges Before Coding**\n",
    "\n",
    "The spatial reasoning required for this problem is difficult. You will need to consider thread and block configurations, shared memory reads and writes, and all in the context of a 2D data set that requires transposition. While this isn't directly concerned with the CUDA programming syntax, it is the kind of problem that can help you become a powerful parallel programmer. Please consider spending time drawing out the problem and your solution on paper before actually trying to write code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing With GPU Errors**\n",
    "\n",
    "For problems with challenging memory access patterns, like this one, it is common to make mistakes. Techniques for digging into errors on the GPU are covered in the section 2 appendix, but you may not have had time to digest them yet, so here are a couple points to help you.\n",
    "\n",
    "* Because we are working out of a Jupyter environment. If you get an error in your CUDA code, you may need to restart the Jupyter kernel. You can do this at any time by using the *Kernel* menu above and selecting *Restart*. This will clear local memory, so you will need to rerun import and code definition files after doing so.\n",
    "* If you get an `UNKNOWN_CUDA_ERROR` it is likely an issue with memory access. Restart the Jupyter kernel, and think through where your code may be making out of range memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Code That Can Handle Multiple Input Sizes**\n",
    "\n",
    "When running the assessment, your code will be assessed against several different input sizes, so you will need to write a kernel that can robustly handle arbitrary input sizes. After successfully solving for the input values below, and before running the assessment, it is recommended you modify the input values (see comment in the cell below) to make sure your code works when changing input sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You do not need to edit the values in this cell, however, before running the assessment\n",
    "# it is recommended you try your code against different input sizes to make sure it can\n",
    "# handle arbitrary input sizes.\n",
    "M = 128\n",
    "N = 32\n",
    "\n",
    "# Consider using this value for N before running the assessment\n",
    "# to make sure your code handles arbitrary input sizes.\n",
    "# N = 8\n",
    "\n",
    "# Input vectors of MxN and NxM dimensions\n",
    "a = np.arange(M*N).reshape(M,N).astype(np.int32)\n",
    "b = np.arange(M*N).reshape(N,M).astype(np.int32)\n",
    "c = np.zeros((M, M)).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "# NxN threads per block, in 2 dimensions\n",
    "block_size = (N,N)\n",
    "# MxM/NxN blocks per grid, in 2 dimensions\n",
    "grid_size = (int(M/N),int(M/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making any modifications to `mm_shared` in the cell below, and before running the assessment, paste this cell's content into [**`assessment/definition.py`**](../edit/assessment/definition.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, types\n",
    "@cuda.jit\n",
    "def mm_shared(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    TPB = N\n",
    "#     sA = cuda.shared.array(shape=(TPB, TPB), dtype=types.float32)\n",
    "#     sB = cuda.shared.array(shape=(TPB, TPB), dtype=types.float32)\n",
    "\n",
    "#     x, y = cuda.grid(2)\n",
    "\n",
    "#     tx = cuda.threadIdx.x\n",
    "#     ty = cuda.threadIdx.y\n",
    "#     bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "#     if x >= C.shape[0] and y >= C.shape[1]:\n",
    "#         # Quit if (x, y) is outside of valid C boundary\n",
    "#         return\n",
    "\n",
    "#     # Each thread computes one element in the result matrix.\n",
    "#     # The dot product is chunked into dot products of TPB-long vectors.\n",
    "#     tmp = 0.\n",
    "#     for i in range(bpg):\n",
    "#         # Preload data into shared memory\n",
    "#         sA[tx, ty] = A[x, ty + i * TPB]\n",
    "#         sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "#         # Wait until all threads finish preloading\n",
    "#         cuda.syncthreads()\n",
    "\n",
    "#         # Computes partial product on the shared memory\n",
    "#         for j in range(TPB):\n",
    "#             tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "#         # Wait until all threads finish computing\n",
    "#         cuda.syncthreads()\n",
    "\n",
    "#     C[x, y] = tmp\n",
    "\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=types.float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=types.float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "#     tmp = float32(0.)\n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx+i*TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty+i*TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no need to update this kernel launch\n",
    "mm_shared[grid_size, block_size](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the contents in this cell\n",
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now that you have completed this section you are able to:\n",
    "\n",
    "* Do GPU accelerated parallel work on multidimensional data sets using multi dimensional blocks and grids.\n",
    "* * Use shared memory to cache data on chip and reduce slow global memory accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Content\n",
    "\n",
    "To download the contents of this notebook, execute the following cell and then click the download link below. Note: If you run this notebook on a local Jupyter server, you can expect some of the file path links in the notebook to be broken as they are shaped to our own platform. You can still navigate to the files through the Jupyter file navigator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tar -zcvf section3.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download the files for this section.](files/section3.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Bank Conflict Free Matrix Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory is stored in **banks**. There are 32 banks available for storing shared memory, and memory reads and writes that do not use the same memory bank can perform simultaneously. When parallel threads attempt to access memory in the same bank, we call this a **bank conflict**, which results in the operations being serialized. Even with bank conflicts, shared memory is very fast, but creating memory access patterns that avoid bank conflict are a way to further optimize your applications.\n",
    "\n",
    "We will only give an example here, but for more details, consider the [Efficient Matrix Transpose](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import cuda, types\n",
    "import numpy as np\n",
    "\n",
    "TILE_DIM = 32\n",
    "BLOCK_ROWS = 8\n",
    "TILE_DIM_PADDED = TILE_DIM + 1  # Read Mark Harris' blog post to find out why this improves performance!\n",
    "                                # https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose_no_bank_conflict(a_in, a_out):\n",
    "    # THIS CODE ASSUMES IT IS RUNNING WITH A BLOCK DIMENSION OF (TILE_SIZE x TILE_SIZE)\n",
    "    # AND INPUT IS A MULTIPLE OF TILE_SIZE DIMENSIONSx\n",
    "    tile = cuda.shared.array((TILE_DIM, TILE_DIM_PADDED), types.int32)\n",
    "\n",
    "    x = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        tile[cuda.threadIdx.y + j, cuda.threadIdx.x] = a_in[y + j, x] # move tile into shared memory\n",
    "\n",
    "    cuda.syncthreads()  # wait for all threads in the block to finish updating shared memory\n",
    "\n",
    "    # Compute transposed offsets\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.y\n",
    "\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        a_out[y + j, x] = tile[cuda.threadIdx.x, cuda.threadIdx.y + j];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 8192\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_shape = (int(size/TILE_DIM), int(size/TILE_DIM))\n",
    "\n",
    "%timeit tile_transpose_no_bank_conflict[grid_shape,(TILE_DIM, BLOCK_ROWS)](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
