{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-rebecca",
   "metadata": {},
   "source": [
    "# Deep Learning from the Basics\n",
    "## Python and Deep Learning: Theory and Implementation\n",
    "\n",
    "#### Koki Saitoh\n",
    "#### Packt Publishing 2021\n",
    "\n",
    "- Data/Code available on GitHub, link in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-pointer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-credits",
   "metadata": {},
   "source": [
    "## Stated Learning Objectives\n",
    "- Use Python with minimum external libraries to implement DL programs\n",
    "- Study various DL and NN theories\n",
    "- Learn how to set initial values of weights\n",
    "- Implement techniques such as batch normalization, dropout, and Adam\n",
    "- Explore applications like automatic driving, image generation, and reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-running",
   "metadata": {},
   "source": [
    "## Target Audience\n",
    "- Data Scientists\n",
    "- Data Analysts\n",
    "- Developers\n",
    "\n",
    "...who want to use DL to develop efficient solutions\n",
    "\n",
    "- \"This book is ideal for those who want a deeper understanding as well as an overview of the techs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-competition",
   "metadata": {},
   "source": [
    "## Prerequisite Knowledge\n",
    "- Some working knowledge of Python is a must\n",
    "- numpy/pandas beneficial but not necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-somewhere",
   "metadata": {},
   "source": [
    "## Getting Started Notes/Impressions\n",
    "- The book clearly states its goal, to implement DL algorithms from scratch\n",
    "- The book clearly states its focus is on image recognition and not other areas\n",
    "- It also clearly states that it is not utilizing standard Python DL packages/frameworks\n",
    "- It also says it is not focusing on GPU usage, model tuning, or latest research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-gather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "final-thriller",
   "metadata": {},
   "source": [
    "# Ch 1: Intro to Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-membership",
   "metadata": {},
   "source": [
    "- Into to Python -> stated to skip if already familiar with Python\n",
    "- Basic intro to Python that seems to cover the most basic info including very basic intro to classes and OOP\n",
    "- Does mention using numpy/matplotlib, but leaves out `pip install` or `conda install` directions\n",
    "- I mostly skimmed this chapter, but it does seem like a decent intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-thong",
   "metadata": {},
   "source": [
    "# Ch 2: Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-communication",
   "metadata": {},
   "source": [
    "- A logical starting point for DL basics\n",
    "- Does a good job of representing how perceptrons work, describing weight/bias\n",
    "- Explains how multi-layer perceptrons build on simple principles to represent more complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-frost",
   "metadata": {},
   "source": [
    "# Ch 3: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-marine",
   "metadata": {},
   "source": [
    "- Focuses on forward propagation\n",
    "- Explains NN layout: input layer, hidden layer(s), output layer\n",
    "- Addresses how activation functions impact output from a node\n",
    "- Explains why non-linear functions must be used as activation functions -> linear activation functions can be represented by a single layer NN\n",
    "    - So you lose the advantage gained by multiple layers\n",
    "- Talks about step, sigmoid, and ReLU activation functions\n",
    "- Goes over matrix multiplication\n",
    "- Implements a three layer NN using matrix multiplication\n",
    "- Introduces activation functions for output layers and common choices: identity/regression, sigmoid/2-class, softmax/multi-class\n",
    "- Explains why output of the softmax function can be interpreted as probability\n",
    "- Says that the softmax function is often omitted from the Output Layer (doesn't change the order of probabilities, meaning which class is the 'answer')\n",
    "- Explains how to determine the number of output nodes (neurons) (equal to the number of classes for classification)\n",
    "- Sort of explains that the number of input layers = the number of features with MNIST data (flattened array length)\n",
    "- Effectively explains batching and why this accelerates the calculation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-magazine",
   "metadata": {},
   "source": [
    "# Ch 4: Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-homework",
   "metadata": {},
   "source": [
    "- Distinguishes between characteristics of how ML & DL learn from data\n",
    "- Good intro to train/test (generalization) and overfitting\n",
    "- Decent explanation of Loss Function\n",
    "- Decent examples of calculating loss function\n",
    "- Introduces gradient descent and describes how it works before mentioning the term\n",
    "- Brings into the fold the learning rate and how it applies by adjusting the amount to update during each iteration of the gradient method\n",
    "    - Also mentions that checking whether or not training is successful can be accomplished by changing the learning rate (0.01 and 0.001 are common)\n",
    "    - Only after describing does it mention the term hyperparameter and talk about modifying it (without using the term hyperparameter tuning)\n",
    "- Lastly describes that Stochastic Gradient Descent (SGD) occurs when using random mini-batches from the training set\n",
    "- Describes that an epoch is the completion of all mini-batches once (all training data has been seen) and indicates the number of iterations through the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-feelings",
   "metadata": {},
   "source": [
    "# Ch 5: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-mileage",
   "metadata": {},
   "source": [
    "- Uses computational graphs (network graphs with nodes/edges) instead of formulas to describe backpropagation\n",
    "- Good job of visually describing how backpropagation finds local differentials\n",
    "- Illustrates how the chain rule applies to back propagation with an example to show how it works\n",
    "- Uses derivatives to explain why backpropagation through an addition node doesn't change the value passed to the lower stream (multiply by 1, derivated of addition only is 1)\n",
    "- Illustrates multiplication backpropagation by reversing the operation (multiplying by x forward means multiplying by y backward and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amber-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch 5 code exercise\n",
    "class MulLayer:\n",
    "    # a multiplication layer with forward and backward operations\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # multiplies the product of two inputs and returns one output\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        # excepts the downstream value (output of forward) and returns the two upstream values (inputs of forward)\n",
    "        dx = dout * self.y # reverse x and y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precise-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# forward prop in a multiplication layer\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "asian-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# backward prop in a multiplication layer\n",
    "# with input of the derivative of the apple price (derivative of 100 = 1)\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "declared-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an addition layer\n",
    "class AddLayer:\n",
    "    def __init__self(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # derivative of addition returns the downstream value (output of forward prop) * 1 to each previous node\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abandoned-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layers to multiply the number*price of apple/oranges, layer to add prices together, layer to multiply the tax\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contained-mission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n"
     ]
    }
   ],
   "source": [
    "# forward prop\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "laden-multimedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# backward prop\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-triple",
   "metadata": {},
   "source": [
    "- Implementing an Activation Function Layer\n",
    "\n",
    "- ReLU\n",
    "    - In forward prop, returns x if x > 0, or 0 if x <= 0\n",
    "    - In back prop, does the same (except this serves as the derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disabled-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relu class\n",
    "\n",
    "# assumes x is a numpy array\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # create a T/F array \n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        \n",
    "        # replaces all x <= 0 values with 0\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # accepts a numpy array of T/F values, sets all True values to 0\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        # essentially turns off the signal if the derivative was 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-budapest",
   "metadata": {},
   "source": [
    "- Complex chapter that really dives into the details for how backpropagation works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-ceramic",
   "metadata": {},
   "source": [
    "# Ch 6: Training Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-engineer",
   "metadata": {},
   "source": [
    "- Discusses optimization techniques and advantages/disadvantages including stochastic gradient descent (SGD), Momentum, AdaGrad\n",
    "- Reinforces the idea that the learning rate is an important hyperparameter (learning rate is how much weights are updated during training)\n",
    "    - Too small and training takes forever\n",
    "    - Too large and divergence occurs and correct training does not occur\n",
    "- Learning rate decay\n",
    "    - Learning rate is larger at first and decreases as training progresses\n",
    "- Disadvantage of SGD\n",
    "    - If the gradient is small is a particular dimention, it is inefficient (folded paper, not much slope to the middle of the paper in one direction)\n",
    "- Momentum\n",
    "    - Helps this problem by reducing the amount of zigzag (takes a shorter path) to get to the local minimum\n",
    "- AdaGrad\n",
    "    - Adjusts the learning rate for each element of the parameter adaptively for training\n",
    "    - If conducted infinitely, the learning rate becomes 0 and no updates occur\n",
    "- RMSProp\n",
    "    - Solves the learning rate to 0 issue with AdaGrad by forgetting past gradients and reduces the scale of past gradients exponentially\n",
    "        - Doesn't reduce the learning rate as much as AdaGrad for each iteration\n",
    "- Adam\n",
    "    - Basic idea is combining Momentem and AdaGrad\n",
    "    - Has characteristic of 'bias correction' for hyperparameters\n",
    "    - Research paper on Adam indicates the hyperparameter values for the primary moment (beta1) and secondary moment (beta2) are often 0.9, and 0.999 respectively, and are effective in many cases\n",
    "    \n",
    "- Explains why initializing with random weights is required (uniform weights will result in improper training using backpropagation)\n",
    "    - Also explains that if wanting weights to start small, use a random normal distribution with a small stdev (`np.random.randn(10, 100) * 0.01`)\n",
    "- Briefly explains the problem of gradient vanishing, where gradients are either 1 or 0\n",
    "    - Shows that this occurred when initial weights had a stdev of 1, fixed by making the initial weights with a stdev of 0.01, but then this causes all of the activations to be more uniform, which negates the value of having multiple neurons\n",
    "        - Activations become biased in this situation resulting in 'limited representation' (distributions are very narrow and all around the same value)\n",
    "    - Explains that the distribution of activations need to be spread properly -> this is efficient learning\n",
    "        - Otherwise you get either gradient vanishing or 'limited representation'\n",
    "    - Talks about the Xavier initializer which are frequently used in ordinary DL frameworks (for tanh/sigmoid activation functions)\n",
    "        - Uses a distribution with a stdev of $\\frac{1}{\\sqrt{n}}$ where `n` is the number of nodes in the previous layer\n",
    "        - This results in activation values for each layer that are more spread out, they still have the same mean, but the stdevs of the activations within the layers are much larger\n",
    "        - Also explains why the tanh function is better than sigmoid (because it's symmetrical about 0, 0 rather than 0, 0.5 like the sigmoid function\n",
    "    - Also states that for ReLU -> the initial value is recommended for initialization\n",
    "        - This is the He intializer\n",
    "        - Gaussian dist with stdev of $\\sqrt{\\frac{2}{n}}$ where n = the number of nodes in the previous layer\n",
    "        - This essentially means that for ReLU, the coefficient must be doubled to provide more spread vs the Xavier initializer (because negative is 0 output for ReLU)\n",
    "        - Results of experiments for weight initializers\n",
    "            - Gaussian stdev of 0.01 -> gradient vanishing and narrow spread (no learning)\n",
    "            - Xavier -> bigger spread, but still some gradient vanishing with higher frequencies around 0 than any other value -> slower training than He\n",
    "            - He -> a high number of 0, but otherwise a flat distribution (even frequencies) for other activation values -> best result\n",
    "- Batch Normalization (batch norm)\n",
    "    - Purpose: to adjust the distribution of activations in each layer so they have a proper spread\n",
    "    - Accelerates learning (can increase the learning rate)\n",
    "    - Not as dependent on initial weigth values (don't need to be cautious with initial values)\n",
    "    - Reduces overfitting (and the need for dropout)\n",
    "    - How it works\n",
    "        - it normalizes each mini-batch used for training (avg = 0, stdev = 1) between layers\n",
    "        - use it either before or after the activation function (some discussion on which is better) to reduce the distribution bias of the data\n",
    "- Regularization (discusses L2 norm only, but mentions L1 and L)\n",
    "    - Weight Decay (adding L2 penalty to the loss function to penalize large weights)\n",
    "        - imposes a penalty on large weights during training (reduces overfitting)\n",
    "    - Dropout\n",
    "        - erases hidden layer neurons at random during training (random different ones each time the data flows)\n",
    "        - during testing, all neurons are used\n",
    "    - Ensemble Models\n",
    "        - similar idea to dropout in that several models are used and predictions are averaged (dropout essentially uses different models each time)\n",
    "        - can improve accuracy by several percent\n",
    "- Validating Hyperparameters\n",
    "    - Essential to not use test data as validation data -> overfitting and test leakage\n",
    "    - Good job explaining that validation data is obtained from the training data\n",
    "    - Reports that random sampling of hyperparameters is a better method for NN's than systematic search (random vs grid search)\n",
    "    - Also mentions that using a log scale (powers of 10) is a good initial approach\n",
    "    - Also mentioned that the size of the epoch for training is often reduced during hyperparameter tuning (search) since trying many options takes a lot of time\n",
    "    - Also mentions trying Bayesian Optimization for hyperparameter tuning and offers a paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-walter",
   "metadata": {},
   "source": [
    "# Ch 7: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-helicopter",
   "metadata": {},
   "source": [
    "- Problem with fully connected layers\n",
    "    - Shape of input data is ignored (3d images: height, width, channel dim) is flattened\n",
    "    - CNN's retain shape, receive shape in the input and also output it -> thus preserving spatial relationships between data points\n",
    "    - Input/output data for a conv. layer is a feature map (input feature map and output feature map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-maximum",
   "metadata": {},
   "source": [
    "# Ch 8: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-charm",
   "metadata": {},
   "source": [
    "- Data Augmentation\n",
    "    - Artificially increases to expand training data (particularly images) by adding new images that are slight modifications of existing images\n",
    "        - Does so by rotation or vertical/horizontal movements\n",
    "        - Can also cut out a part of an image, flip horizontally (only works when symmetry isn't important), changing brightness\n",
    "- Discusses Transfer Learning\n",
    "    - Take part of the trained weights from one pretrained model then fine tune them\n",
    "- Discusses GPUs for LTM data\n",
    "- Also distributed use of GPUs using Google's TensorFlow or MS Computational Network Toolkit (CNTK)\n",
    "- Talks about different areas of DL and their applications moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-filling",
   "metadata": {},
   "source": [
    "# Impressions\n",
    "- Good for someone who is familiar with ML/DL but wants to have a better understanding of the inner-workings of DL/NN\n",
    "- Otherwise good for those strong in math, calculus (particularly derivatives), and vector/matrix math\n",
    "- Would be difficult to comprehend without a background, but does a good job of shedding light on the details of these principles that many are familiar with and really should understand to properly implement DL\n",
    "- Explains the math behind what's occuring to help Data Scientists make more informed decisions on the choice of \n",
    "    - activation functions \n",
    "    - regularization\n",
    "    - hyperparameter tuning\n",
    "    - activation functions\n",
    "    - optimization methods for loss functions\n",
    "- Provides some intro on particular types of DL and areas where they are applied, things to look out for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-scroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
