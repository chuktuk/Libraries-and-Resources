{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics\n",
    "- Suite of algorithms and techniques that learn from data, find patterns, and make predictions\n",
    "- Useful with multivariate systems that are too complicated to use classical statistics\n",
    "- Two main types\n",
    "    - Supervised Learning\n",
    "        - learning from labeled examples\n",
    "    - Unsupervised Learning\n",
    "        - learning from unlabeled data to create categories/clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "- Selection Bias\n",
    "    - where did the data come from, and what are you missing\n",
    "    - your sample vs. the population\n",
    "- Publication Bias\n",
    "    - only positive/significant results published\n",
    "    - highly influence by $\\alpha < 0.05$ threshold\n",
    "- Non-Response Bias\n",
    "    - people did/didn't respond are different\n",
    "- Length Bias\n",
    "    - especially important with time based measurements\n",
    "    - bias towards individuals (data points) that remain in a specific state longer\n",
    "        - you miss the two day sample and collect the 20 yr sample, this happens repeatedly\n",
    "- Calculation\n",
    "    - difference between your estimation and the \"truth\"\n",
    "        - almost impossible to find the \"truth\", so really can't calculate bias accurately\n",
    "- MSE (meas squared error)\n",
    "    - $MSE = variance + bias^2$\n",
    "    - decreasing bias often means taking more samples or increasing model complexity, which means increasing variance\n",
    "    - it's a tradeoff, and decreasing bias may increase variance enough to make the MSE very large\n",
    "    - want to minimize MSE, rather than bias\n",
    "- Fisher Weighting\n",
    "    - way to take multiple independent unbiased estimators and combine into one number\n",
    "    - take a weighted average\n",
    "        - individual weights add up to one\n",
    "        - individual weights inversely proportional to the variance\n",
    "- Nate Silver Weighting\n",
    "    - take each variable into account based on particular circumstances\n",
    "        - past reliability, known biases, etc.\n",
    "- Bonferroni Correction\n",
    "    - divide $\\alpha$/number of tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression toward the mean (RTTM)\n",
    "    - predicting things like son's heights based on father's heights\n",
    "        - the son won't be exactly the same height as the father, and will likely be between the father's height and the mean\n",
    "        - combination of luck/skill (luck = chance, skill = effect)\n",
    "        - example praising good performance leads to worse performance and punishing bad performance leads to better performance:\n",
    "            - RTTM would predict bad to get better and good to get worse in many cases just by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS (Ordinary Least Squares)\n",
    "- defines a linear model (the model is not OLS, that is the procedure to generate the linear model)\n",
    "- **always plot the residuals!**\n",
    "    - no pattern is a good thing\n",
    "    - heteroscedasticity: bigger spread on one side\n",
    "    - nonlinear: distinct pattern to the residuals (U-shape or similar)\n",
    "- Goodness of fit\n",
    "    - $R^2$\n",
    "        - explained or 'accounted for' variance\n",
    "        - amount of variance captured by the model\n",
    "    - bad to use for telling how good a model is at predicting\n",
    "        - $R^2$ will always go up as you add more variables\n",
    "        - cross validation is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Tips\n",
    "- When in doubt, take the log of predictor variables\n",
    "- Avoid collinearity\n",
    "    - using independent variables that are highly correlated with each other\n",
    "    - tends to inflate regression coefficient, create high variances in estimates, and high instability\n",
    "    - variance inflation factor helps to measure collinearity (1-2 very little to no collin. while over 20 is extreme collin.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- Good for predicting binary outcomes\n",
    "    - think of the logistic s curve, the bottom is the first outcome with a value of 0, and the second outcome is the top of the curve with an outcome of 1\n",
    "- Odds ratio\n",
    "    - probability of an outcome is $p$\n",
    "    - odds of the outcome are $p/(1-p)$\n",
    "        - odds = 10/1\n",
    "        - probability: 10 = p/(1-p) -> 10 - 10p = p -> 10 = 11p -> p = 10/11 = ~0.91\n",
    "        - if p is very small, 1-p is about 0 and odds are about equal to prob, but only in this case\n",
    "    - odds ratio = $\\frac{(p_1/(1-p_1))}{(p_2/(1-p_2))}$\n",
    "        - where $p_1$ is the probability of outcome 1 and $p_2$ is the probability of outcome 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
