{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics\n",
    "- Suite of algorithms and techniques that learn from data, find patterns, and make predictions\n",
    "- Useful with multivariate systems that are too complicated to use classical statistics\n",
    "- Two main types\n",
    "    - Supervised Learning\n",
    "        - learning from labeled examples (categories)\n",
    "        - kNN, SVM, Decision Trees, Random Forests, Bagging, Boosting, etc.\n",
    "        - make predictions for new data points\n",
    "    - Unsupervised Learning\n",
    "        - learning from unlabeled data to create categories/clusters\n",
    "        - PCA, MDS, Clustering\n",
    "        - find patterns in data\n",
    "        - for data with no categories (maybe don't know what they are), don't have training data, can't label because it's so big, don't know what's in it, or need to identify patterns/structure\n",
    "- Learn by example\n",
    "    - training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias\n",
    "- Selection Bias\n",
    "    - where did the data come from, and what are you missing\n",
    "    - your sample vs. the population\n",
    "- Publication Bias\n",
    "    - only positive/significant results published\n",
    "    - highly influence by $\\alpha < 0.05$ threshold\n",
    "- Non-Response Bias\n",
    "    - people did/didn't respond are different\n",
    "- Length Bias\n",
    "    - especially important with time based measurements\n",
    "    - bias towards individuals (data points) that remain in a specific state longer\n",
    "        - you miss the two day sample and collect the 20 yr sample, this happens repeatedly\n",
    "- Calculation\n",
    "    - difference between your estimation and the \"truth\"\n",
    "        - almost impossible to find the \"truth\", so really can't calculate bias accurately\n",
    "- MSE (meas squared error)\n",
    "    - $MSE = variance + bias^2$\n",
    "    - decreasing bias often means taking more samples or increasing model complexity, which means increasing variance\n",
    "    - it's a tradeoff, and decreasing bias may increase variance enough to make the MSE very large\n",
    "    - want to minimize MSE, rather than bias\n",
    "- Fisher Weighting\n",
    "    - way to take multiple independent unbiased estimators and combine into one number\n",
    "    - take a weighted average\n",
    "        - individual weights add up to one\n",
    "        - individual weights inversely proportional to the variance\n",
    "- Nate Silver Weighting\n",
    "    - take each variable into account based on particular circumstances\n",
    "        - past reliability, known biases, etc.\n",
    "- Bonferroni Correction\n",
    "    - divide $\\alpha$/number of tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression toward the mean (RTTM)\n",
    "    - predicting things like son's heights based on father's heights\n",
    "        - the son won't be exactly the same height as the father, and will likely be between the father's height and the mean\n",
    "        - combination of luck/skill (luck = chance, skill = effect)\n",
    "        - example praising good performance leads to worse performance and punishing bad performance leads to better performance:\n",
    "            - RTTM would predict bad to get better and good to get worse in many cases just by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS (Ordinary Least Squares)\n",
    "- defines a linear model (the model is not OLS, that is the procedure to generate the linear model)\n",
    "- **always plot the residuals!**\n",
    "    - no pattern is a good thing\n",
    "    - heteroscedasticity: bigger spread on one side\n",
    "    - nonlinear: distinct pattern to the residuals (U-shape or similar)\n",
    "- Goodness of fit\n",
    "    - $R^2$\n",
    "        - explained or 'accounted for' variance\n",
    "        - amount of variance captured by the model\n",
    "    - bad to use for telling how good a model is at predicting\n",
    "        - $R^2$ will always go up as you add more variables\n",
    "        - cross validation is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Tips\n",
    "- When in doubt, take the log of predictor variables\n",
    "- Avoid collinearity\n",
    "    - using independent variables that are highly correlated with each other\n",
    "    - tends to inflate regression coefficient, create high variances in estimates, and high instability\n",
    "    - variance inflation factor helps to measure collinearity (1-2 very little to no collin. while over 20 is extreme collin.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- Basics\n",
    "    - good for predicting binary outcomes\n",
    "    - think of the logistic s curve, the bottom is the first outcome with a value of 0, and the second outcome is the top of the curve with an outcome of 1\n",
    "    - provides a framework for considering/controlling for variables \n",
    "- Odds ratio\n",
    "    - probability of an outcome is $p$\n",
    "    - odds of the outcome are $p/(1-p)$\n",
    "        - odds = 10/1\n",
    "        - probability: 10 = p/(1-p) -> 10 - 10p = p -> 10 = 11p -> p = 10/11 = ~0.91\n",
    "        - if p is very small, 1-p is about 0 and odds are about equal to prob, but only in this case\n",
    "    - odds ratio = $\\frac{(p_1/(1-p_1))}{(p_2/(1-p_2))}$\n",
    "        - where $p_1$ is the probability of outcome 1 and $p_2$ is the probability of outcome 2\n",
    "- Strategy\n",
    "    - use Maximum Likelihood Estimation (MLE) to estimate parameters from the data\n",
    "    - define variables\n",
    "        - y is 0 or 1 (false or true) for the variable you are trying to predict\n",
    "        - can have multiple x variables\n",
    "            - can be binary 0/1 (presence/absence, gender, etc.)\n",
    "            - can be continuous (age, etc.)\n",
    "    - want the probability of Y given your X variables\n",
    "        - $p = P(Y=1|X_1, X_2, ..., X_k)$\n",
    "    - logistic regression model (logit)\n",
    "        - $logit(p) = ln (\\frac{p}{1-p}) = \\beta_0 + \\beta_1X_1 + ... + \\beta_k X_k$\n",
    "    - once the $\\beta$ values are calculated above, you can plugin values for your $X$ variables to calculate probabilities and make predictions\n",
    "    - interpret results (adjusted odds ratio)\n",
    "        - Example:\n",
    "        - model calculates a $\\beta$ value of 1.89 for a binary X variable\n",
    "        - plug into equations and simplify to get an 'adjusted odds ratio' = $e^{\\beta_k} = ~ 6.3$\n",
    "            - (raise e to the $\\beta$ power) to calculate addjusted odds ratio\n",
    "        - the '1' state has increased odds of 6.3 compared to the '0' state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of Dimensionality\n",
    "- As the number of dimensions increases, the amount of 'space' in the dimension increases rapidly, while the probability space can't keep up\n",
    "    - this results in an exponential decrease in probability with dimension\n",
    "    - i.e. a circle tangent to a square takes up 0.79 of the volume of the dimension space, while a sphere tangent to a cube takes up just 0.52 of the volume of the dimension space\n",
    "        - by dimension 6 this is just 0.08 and 10 is 0.002\n",
    "- Other descriptions\n",
    "    - overfitting a model\n",
    "    - harder to cluster data, individual points seem like their own cluster (overfitting)\n",
    "    - model parameters can reach 100's or 1000's and even have more parameters than data points\n",
    "    - **most statistics were designed to describe a central tendency (the middle)**\n",
    "        - the numbers above are the random chance of a point in that space being 'central'\n",
    "        - at 100 dimensions this goes down to $1.87 * 10^{-70}$\n",
    "    - in high dimensions, there is often no choice but extrapolation\n",
    "        - the space is so sparse that you must look outside the estimate to predict (finding close neighbor points)\n",
    "- Interpolation is better than Extrapolation in many cases (straight line not always the best answer)\n",
    "    - interpolation -> new (predicted) data points are within the range of observed points\n",
    "    - exprapolation -> new (predicted) data points are beyond the range of observed points\n",
    "- Blessing, not a curse?\n",
    "    - Gelman, uses a hierarchy to give weights to the variables, and this helps to give order to the model, supposedly aggregating neighbors better within the predictive space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with 'Wide' Data\n",
    "- wide data = lots of variables, whereas tall data has lots of observations\n",
    "    - table shape\n",
    "- Ridge Regression\n",
    "    - an extension of linear regression\n",
    "    - takes the sum of the squared residuals between observed/model + the sum of squared $\\beta$'s times $\\lambda$ which is a tuning parameter \n",
    "        - (penalty factor)\n",
    "        - imposes a penalty to the model related to the number of variables used (to promote dimensionality reduction)\n",
    "    - there are other types of regression that also punish too high dimensionality starting with OLS, then adding a penalty factor\n",
    "- Shrinkage Estimation\n",
    "    - not out of Seinfeld\n",
    "    - **Stein's Paradox**\n",
    "        - use total MSE to calculate performance (standard loss function)\n",
    "        - try to estimate 3 or more means for predicting 3 or more independent y values (dependent variables)\n",
    "        - prediction estimates tend to shrink towards a grand mean (sort of like regression towards the mean)\n",
    "        - makes the most sense when 'independence' may not be complete (i.e. many different baseball players' avgs.)\n",
    "- LASSO and Sparsity\n",
    "    - helps to induce sparsity, which reduces dimensions\n",
    "        - will reduce hierarchy of $\\beta$'s to 0 (reduces the number of variables that matter)\n",
    "    - like Ridge regression, but penalty term is sum of $\\lambda * abs(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Assigning labels to data\n",
    "- Examples: \n",
    "    - Email -> spam or not spam\n",
    "    - Text -> which language is it\n",
    "    - Images -> classify images as a type or search for features\n",
    "- Greatly enhanced by machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Strategy\n",
    "- Input:\n",
    "    - training dataset of N data points, each labeled as one of K different classes\n",
    "- Learn:\n",
    "    - use the training dataset to learn about the classes\n",
    "- Evaluate:\n",
    "    - predict the labels for a test set and compare to the true labels (ground truthing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Explanation\n",
    "- k number 'features' are used to essentially plot the data in a k dimensional space\n",
    "- we are looking for the decision boundary(ies) between the different classes\n",
    "- data points are typically called 'x' and the labels called 'y'\n",
    "- depends heavily on feature selection\n",
    "    - crappy features will result in overlap and no real decision boundary\n",
    "    - good features can result in obvious decision boundaries\n",
    "    - cross validation helps with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor Classification\n",
    "- Simplest way to classify\n",
    "    - 'majority voting' kind of deal\n",
    "    - look at the categories of the nearest datapoints: assign predicted categories based on the known categories that are closest in feature values to the prediction data point\n",
    "    - decisions are impacted on the 'size' of the neighborhood\n",
    "        - evaluate the performance of the classifier to pick the neighborhood size\n",
    "        - this is usually not a set size, but instead the value 'k' which is the number of neighbors to 'poll' (typically an odd number so no ties)\n",
    "- 1 nearest neighbor properties\n",
    "    - rough decision boundary\n",
    "    - will draw islands around points that are the 'wrong category' in another category's space\n",
    "        - no bueno when this happens\n",
    "    - however, it is simple and often quite good for well separated, low dimension data (few features)\n",
    "        - especially when there are no 'island' points in the 'wrong spot'\n",
    "    - complexity\n",
    "        - complexity of adding N points to the training set is order 1\n",
    "            - just add the additional data points/labels (N more calculations)\n",
    "        - complexity of new test data M number of points\n",
    "            - complexity is M x N, must go through every training point for each new test point\n",
    "        - weird tradeoff\n",
    "            - ideally you spend your time during training, so testing is quick, but this version is opposite that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
